mpiexec (OpenRTE) 3.1.4

Report bugs to http://www.open-mpi.org/community/help/
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[42611,1],2] (PID 38995)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
[cn10-09:33994] 3 more processes have sent help message help-opal-runtime.txt / opal_init:warn-fork
[cn10-09:33994] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
2023-12-31 12:35:57,394 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-31 12:35:57,501 - distributed.scheduler - INFO - State start
2023-12-31 12:35:57,506 - distributed.scheduler - INFO -   Scheduler at:     tcp://10.1.10.9:45058
2023-12-31 12:35:57,507 - distributed.scheduler - INFO -   dashboard at:  http://10.1.10.9:8787/status
2023-12-31 12:35:57,548 - distributed.worker - INFO -       Start worker at:     tcp://10.1.10.10:46177
2023-12-31 12:35:57,548 - distributed.worker - INFO -          Listening to:     tcp://10.1.10.10:46177
2023-12-31 12:35:57,548 - distributed.worker - INFO -           Worker name:                          3
2023-12-31 12:35:57,548 - distributed.worker - INFO -          dashboard at:           10.1.10.10:39500
2023-12-31 12:35:57,548 - distributed.worker - INFO - Waiting to connect to:      tcp://10.1.10.9:45058
2023-12-31 12:35:57,548 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 12:35:57,548 - distributed.worker - INFO -               Threads:                          1
2023-12-31 12:35:57,548 - distributed.worker - INFO -                Memory:                   0.98 GiB
2023-12-31 12:35:57,548 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kyyrob12
2023-12-31 12:35:57,548 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 12:35:57,548 - distributed.worker - INFO -       Start worker at:     tcp://10.1.10.10:36429
2023-12-31 12:35:57,548 - distributed.worker - INFO -          Listening to:     tcp://10.1.10.10:36429
2023-12-31 12:35:57,549 - distributed.worker - INFO -           Worker name:                          2
2023-12-31 12:35:57,549 - distributed.worker - INFO -          dashboard at:           10.1.10.10:35670
2023-12-31 12:35:57,549 - distributed.worker - INFO - Waiting to connect to:      tcp://10.1.10.9:45058
2023-12-31 12:35:57,549 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 12:35:57,549 - distributed.worker - INFO -               Threads:                          1
2023-12-31 12:35:57,549 - distributed.worker - INFO -                Memory:                   0.98 GiB
2023-12-31 12:35:57,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kw4ty3eq
2023-12-31 12:35:57,550 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 12:35:58,173 - distributed.scheduler - INFO - Receive client connection: Client-c3d99617-a7d0-11ee-84db-d06726cd5682
2023-12-31 12:35:58,176 - distributed.core - INFO - Starting established connection to tcp://10.1.10.9:54876
2023-12-31 12:35:59,726 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.1.10.10:36429', name: 2, status: init, memory: 0, processing: 0>
2023-12-31 12:35:59,727 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.10.10:36429
2023-12-31 12:35:59,727 - distributed.core - INFO - Starting established connection to tcp://10.1.10.10:60866
2023-12-31 12:35:59,728 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.1.10.10:46177', name: 3, status: init, memory: 0, processing: 0>
2023-12-31 12:35:59,728 - distributed.worker - INFO -         Registered to:      tcp://10.1.10.9:45058
2023-12-31 12:35:59,728 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 12:35:59,729 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.10.10:46177
2023-12-31 12:35:59,729 - distributed.core - INFO - Starting established connection to tcp://10.1.10.10:60864
2023-12-31 12:35:59,729 - distributed.core - INFO - Starting established connection to tcp://10.1.10.9:45058
2023-12-31 12:35:59,729 - distributed.worker - INFO -         Registered to:      tcp://10.1.10.9:45058
2023-12-31 12:35:59,730 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 12:35:59,731 - distributed.core - INFO - Starting established connection to tcp://10.1.10.9:45058
slurmstepd: error: *** JOB 11008057 ON cn10-09 CANCELLED AT 2023-12-31T13:17:28 DUE TO TIME LIMIT ***
--------------------------------------------------------------------------
ORTE has lost communication with a remote daemon.

  HNP daemon   : [[42611,0],0] on node cn10-09
  Remote daemon: [[42611,0],2] on node cn10-11

This is usually due to either a failure of the TCP network
connection to the node, or possibly an internal failure of
the daemon itself. We cannot recover from this failure, and
therefore will terminate the job.
--------------------------------------------------------------------------
