mpiexec (OpenRTE) 3.1.4

Report bugs to http://www.open-mpi.org/community/help/
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[38406,1],3] (PID 52948)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
[cn10-20:61743] 3 more processes have sent help message help-opal-runtime.txt / opal_init:warn-fork
[cn10-20:61743] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
2023-12-31 11:20:00,207 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-31 11:20:00,304 - distributed.scheduler - INFO - State start
2023-12-31 11:20:00,308 - distributed.scheduler - INFO -   Scheduler at:    tcp://10.1.10.20:46543
2023-12-31 11:20:00,309 - distributed.scheduler - INFO -   dashboard at:  http://10.1.10.20:8787/status
2023-12-31 11:20:00,349 - distributed.worker - INFO -       Start worker at:     tcp://10.1.10.21:38275
2023-12-31 11:20:00,349 - distributed.worker - INFO -          Listening to:     tcp://10.1.10.21:38275
2023-12-31 11:20:00,349 - distributed.worker - INFO -           Worker name:                          3
2023-12-31 11:20:00,349 - distributed.worker - INFO -          dashboard at:           10.1.10.21:39756
2023-12-31 11:20:00,349 - distributed.worker - INFO - Waiting to connect to:     tcp://10.1.10.20:46543
2023-12-31 11:20:00,349 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 11:20:00,349 - distributed.worker - INFO -               Threads:                          1
2023-12-31 11:20:00,349 - distributed.worker - INFO -       Start worker at:     tcp://10.1.10.21:33412
2023-12-31 11:20:00,349 - distributed.worker - INFO -                Memory:                   0.98 GiB
2023-12-31 11:20:00,349 - distributed.worker - INFO -          Listening to:     tcp://10.1.10.21:33412
2023-12-31 11:20:00,349 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_l3ms7_x
2023-12-31 11:20:00,350 - distributed.worker - INFO -           Worker name:                          2
2023-12-31 11:20:00,350 - distributed.worker - INFO -          dashboard at:           10.1.10.21:45087
2023-12-31 11:20:00,350 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 11:20:00,350 - distributed.worker - INFO - Waiting to connect to:     tcp://10.1.10.20:46543
2023-12-31 11:20:00,350 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 11:20:00,350 - distributed.worker - INFO -               Threads:                          1
2023-12-31 11:20:00,350 - distributed.worker - INFO -                Memory:                   0.98 GiB
2023-12-31 11:20:00,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ewk7mvql
2023-12-31 11:20:00,350 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 11:20:00,967 - distributed.scheduler - INFO - Receive client connection: Client-27912468-a7c6-11ee-b140-d06726cd5722
2023-12-31 11:20:00,970 - distributed.core - INFO - Starting established connection to tcp://10.1.10.20:38994
2023-12-31 11:20:02,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.1.10.21:33412', name: 2, status: init, memory: 0, processing: 0>
2023-12-31 11:20:02,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.10.21:33412
2023-12-31 11:20:02,467 - distributed.core - INFO - Starting established connection to tcp://10.1.10.21:38132
2023-12-31 11:20:02,467 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.1.10.21:38275', name: 3, status: init, memory: 0, processing: 0>
2023-12-31 11:20:02,467 - distributed.worker - INFO -         Registered to:     tcp://10.1.10.20:46543
2023-12-31 11:20:02,467 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 11:20:02,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.10.21:38275
2023-12-31 11:20:02,469 - distributed.core - INFO - Starting established connection to tcp://10.1.10.21:38130
2023-12-31 11:20:02,468 - distributed.core - INFO - Starting established connection to tcp://10.1.10.20:46543
2023-12-31 11:20:02,469 - distributed.worker - INFO -         Registered to:     tcp://10.1.10.20:46543
2023-12-31 11:20:02,469 - distributed.worker - INFO - -------------------------------------------------
2023-12-31 11:20:02,470 - distributed.core - INFO - Starting established connection to tcp://10.1.10.20:46543
slurmstepd: error: *** JOB 11007030 ON cn10-20 CANCELLED AT 2023-12-31T11:31:46 DUE TO TIME LIMIT ***
