{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d184cd",
   "metadata": {},
   "source": [
    "# <span style=\"color:#8B4513;\"> Machine Learning and RNA-Seq Data of Parkinson Disease\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "[<span style=\"color:#8B4513;\">Author: **Zainab Nazari**</span>](mailto:z.nazari@ebri.com)\n",
    " \n",
    " <span style=\"color:#8B4513;\">EBRI – European Brain Research Institute Rita Levi-Montalcini | MHPC - Master in High Performance Computing</span>\n",
    " \n",
    "\n",
    "\n",
    "## Introduction\n",
    "By employing machine learning in PPMI clinical data set, we can develop predictive models that aid in the early diagnosis of the disease. These models can potentially identify specific genetic markers or gene signatures that correlate with disease progression or response to treatment.\n",
    "\n",
    "## Table of Contents\n",
    "- [Matrix of Gene IDs and Counts for Pateints](#matrixcreation)\n",
    "- [Data Preprocessing STEP I](#preprocessing)\n",
    "- [Data Preprocessing STEP II](#preprocessing2)\n",
    "- [Model Training](#training)\n",
    "- [Results and Evaluation](#results)\n",
    "\n",
    "## Matrix of Gene IDs and Counts for Pateints\n",
    "- Loading the data from IR3/counts folder and extracting the associated last column (counts) of each patient file for their BL visit.\n",
    "\n",
    "\n",
    "## Data Preprocessing STEP I\n",
    "- We remove patients that have these diseases: SNCA (ENRLSNCA), GBA (ENRLGBA), LRRK2 (ENRLLRRK2).\n",
    "-  We only keep genes with the intersection of counts and quants with proteing coding and RNAincs.\n",
    "- We remove the duplicated gene IDs in which they are also lowly expressed.\n",
    "- We keep only patients with diagnosis of Health control or Parkinson disease.\n",
    "- We check if there are some patients were they were taking dopamine drug, so we exclude them. Dopaminergic medication can impact the interpretation of experimental data or measurements and can alter gene expression patterns, so we need to remove them to have less biased data.\n",
    "\n",
    "## Data Preprocessing STEP II\n",
    "1. We remove lowely expressed genes, by keeping only genes that had more than five counts in at least 10% of the individuals, which left us with 21,273 genes\n",
    "\n",
    "2. Similar DESeq2 but with numpy:  we estimated size factors, normalized the library size bias using these factors, performed independent filtering to remove lowly expressed genes using the mean of normalized counts as a filter statistic. This left us with 22969 genes\n",
    "\n",
    "3. pyDESeq2: we apply a variance stabilizing transformation (vst) to accommodate the problem of unequal variance across the range of mean values.\n",
    "\n",
    "\n",
    "4. limma: we used control samples to estimate the batch effect of the site, that we subsequently removed in both controls and cases. In experimental research, a batch effect is a systematic variation in data that can occur when data is collected from multiple sites (clinical centers). These factors can include differences in equipment, reagents, operators, or experimental conditions. Examples of batch effects: \n",
    " - Differences in the equipment used to collect the data. For example, if you are using two different microarray platforms to measure gene expression, there may be differences in the way that the platforms detect and quantify gene expression.\n",
    " - Differences in the operators who collect the data. For example, if two different people are collecting RNA-seq data, they may have different levels of experience or expertise, which could lead to differences in the way that they process the samples.\n",
    " \n",
    "\n",
    "5. using limma: we removed further confounding effects due to sex and RIN value. RIN value is a measure of the quality of RNA samples, and it can vary depending on the sample preparation method. Sex can also affect gene expression. If the effects of sex and RIN value are not removed, then the results of the analysis may be biased.\n",
    "\n",
    "\n",
    "## Model Training\n",
    "We bulid a randm forest with 100 number of deccision treas, we split the training and testing and build the random forest model 20 times and find the average.\n",
    "\n",
    "## Results and Evaluation\n",
    "We present the results of the trained models, including performance metrics, accuracy, or any relevant evaluation measures. The model without preprocessing is with high recall score and low roc and auc score, and this means that the model is good to distinguishing the person with parkinson but not healthy people, therefore the model sounds very random.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "Summarize the key findings, limitations of the analysis, and potential future work or improvements. Offer closing remarks or suggestions for further exploration.\n",
    "\n",
    "## References\n",
    "- [**Parkinson’s Progression Markers Initiative (PPMI)**](https://www.ppmi-info.org/)\n",
    "\n",
    "- [**A Machine Learning Approach to Parkinson’s Disease Blood Transcriptomics**](https://www.mdpi.com/2073-4425/13/5/727)\n",
    "\n",
    "- [**Quality Control Metrics for Whole Blood Transcriptome Analysis in the Parkinson’s Progression Markers Initiative (PPMI)**](https://www.medrxiv.org/content/10.1101/2021.01.05.21249278v1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7b8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you do not have following packages installed, uncomment instalisation.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip install dask[complete];\n",
    "# you need to run these in case dask gives you error, it might need update.\n",
    "#!pip install --upgrade pandas \"dask[complete]\"\n",
    "#python -m pip install \"dask[dataframe]\" --upgrade\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#!pip3 install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#!pip install conorm\n",
    "import conorm # for tmm normalisation\n",
    "\n",
    "#!pip3 install pydeseq2 or pip install pydeseq2\n",
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.ds import DeseqStats\n",
    "from pydeseq2.utils import load_example_data\n",
    "\n",
    "\n",
    "\n",
    "#to install R :\n",
    "#conda install -c r r-irkernel\n",
    "\n",
    "#to install a library from R\n",
    "#!pip install library edgeR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba5df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the counts file in the IR3 is around 152 G, and the files are located in scratch area.\n",
    "\n",
    "path_to_files=\"/scratch/znazari/PPMI_ver_sep2022/RNA_Seq_data/star_ir3/counts/\"\n",
    "path1=Path(\"/scratch/znazari/PPMI_ver_sep2022/RNA_Seq_data/star_ir3/counts/\")\n",
    "path2 = Path(\"/home/znazari/data\") # where the output data will be saved at the end.\n",
    "path3=Path(\"/scratch/znazari/PPMI_ver_sep2022/study_data/Subject_Characteristics/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86494e1a",
   "metadata": {},
   "source": [
    "<a id=\"matrixcreation\"></a>\n",
    "## Matrix of Gene IDs and Counts for Pateints\n",
    " Loading the data from IR3/counts folder and extracting the associated last column (counts) of each patient file for their BL visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b8db8",
   "metadata": {
    "tags": [
     "matrix-creation"
    ]
   },
   "outputs": [],
   "source": [
    "#reading the files which are in BL (Base line) visit.\n",
    "specific_word = 'BL'\n",
    "ending_pattern = '*.txt'\n",
    "file_pattern = f'*{specific_word}*.{ending_pattern}'\n",
    "file_paths = glob.glob(path_to_files + file_pattern)\n",
    "# 'bl.txt' is a file that ccontains the name of the files with patient, BL, IR3, counts.\n",
    "filename = 'bl.txt'\n",
    "file_path_2 = os.path.join(path_to_files, filename)\n",
    "bl_files = pd.read_csv(file_path_2,header=None)\n",
    "\n",
    "# We define a function where we can take the second phrase seperated by dot. The second phrase \n",
    "# is the patient ID. So with this functin we want to get the patient IDs from their file's name\n",
    "def function_names(fname):\n",
    "    tokens=fname.split('.')\n",
    "    return tokens[1]\n",
    "\n",
    "# we create a list with the name of the each patients.\n",
    "bl_list = [function_names(bl_files.iloc[i][0]) for i in range(len(bl_files))]\n",
    "\n",
    "# here we read all the files with with base visit(BL) from the counts folder (where we have all the files\n",
    "# for all the patients and all the visit).\n",
    "list_bl_files = [dd.read_csv(path1/bl_files.iloc[i][0],skiprows=1,delimiter='\\t') for i in range(len(bl_files))]\n",
    "\n",
    "\n",
    "# we get th last columns of each file in the list\n",
    "last_columns = [ddf.iloc[:, -1:] for ddf in list_bl_files]\n",
    "\n",
    "# concatinating the list of the columns in a single file.\n",
    "single_file = dd.concat(last_columns, axis=1)\n",
    "\n",
    "# we change the name of the each columns with the patient numbers.\n",
    "single_file.columns = bl_list\n",
    "\n",
    "# we get the Geneid column and convert it to dask dataframe\n",
    "pd_tmp_file = list_bl_files[3].compute()\n",
    "geneid = pd_tmp_file['Geneid']\n",
    "ddf_geneid = dd.from_pandas(geneid, npartitions=1)\n",
    "\n",
    "# here we set the Geneid column as the index of the matrix.\n",
    "ddf_new_index = single_file.set_index(ddf_geneid)\n",
    "\n",
    "# converting to pandas data frame and saving.\n",
    "ir3_counts = ddf_new_index.compute()\n",
    "ir3_counts.to_csv(path2/\"matrix_ir3_counts_bl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab10c6",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "## Data Preprocessing STEP I\n",
    "\n",
    "- We remove patients that have these diseases: SNCA (ENRLSNCA), GBA (ENRLGBA), LRRK2 (ENRLLRRK2).\n",
    "- dopamin drug using\n",
    "-  We only keep genes with the intersection of counts and quants with proteing coding and RNAincs.\n",
    "- We remove the duplicated gene IDs in which they are also lowly expressed.\n",
    "- We keep only patients with diagnosis of Health control or Parkinson disease.\n",
    "- We check if there are some patients were they were taking dopomine drug, so we exclude them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07749200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file\n",
    "read_ir3_counts = pd.read_csv(path2/\"matrix_ir3_counts_bl.csv\")\n",
    "# setting the geneid as indexing column\n",
    "read_ir3_counts.set_index('Geneid', inplace=True)\n",
    "# result with removing the after dot (.) value, i.e. the version of the geneIDs is removed.\n",
    "read_ir3_counts.index =read_ir3_counts.index.str.split('.').str[0]\n",
    "\n",
    "\n",
    "#here we delete the duplicated gene IDs, first we find them then remove them from the gene IDs\n",
    "# as they are duplicated and also they are very lowly expressed either zero or one in rare caes.\n",
    "\n",
    "# Check for duplicate index values\n",
    "is_duplicate = read_ir3_counts.index.duplicated()\n",
    "\n",
    "# Display the duplicate index values\n",
    "duplicate_indices = read_ir3_counts.index[is_duplicate]\n",
    "\n",
    "# drop them (duplicated indices and their copies are deleted, 45 duplicatd indices and 90 are dropped)\n",
    "to_be_deleted = list(duplicate_indices)\n",
    "read_ir3_counts = read_ir3_counts.drop(to_be_deleted)\n",
    "\n",
    "# we read the file where we have an intersection of geneIDs in IR3, counts, quant\n",
    "intersect = pd.read_csv(path2/\"intersect_IR3_ENG_IDs_LincRNA_ProtCoding_counts_quant_gene_transcript_only_tot_intsersect.txt\")\n",
    "intersection = read_ir3_counts.index.intersection(intersect['[IR3_gene_counts] and [IR3_quant_gene] and [IR3_quant_trans] and [lncRNA+ProtCod]: '])\n",
    "filtered_read_ir3_counts = read_ir3_counts.loc[intersection]\n",
    "\n",
    "# reading the file which contains diagnosis\n",
    "diago=pd.read_csv(path3/\"Participant_Status.csv\", header=None )\n",
    "diago1=diago.rename(columns=diago.iloc[0]).drop(diago.index[0]).reset_index(drop=True)\n",
    "\n",
    "#this is to remove patients that have these diseases: SNCA (ENRLSNCA), GBA (ENRLGBA), LRRK2 (ENRLLRRK2)\n",
    "filtered_SNCA_GBA_LRRK2 = diago1[(diago1['ENRLSNCA'] == \"0\")& (diago1['ENRLGBA'] == \"0\")& (diago1['ENRLLRRK2'] == \"0\")]\n",
    "\n",
    "#patients with their diagnosis\n",
    "patinets_diagnosis = filtered_SNCA_GBA_LRRK2[['PATNO','COHORT_DEFINITION']].reset_index(drop=True)\n",
    "\n",
    "# Define the particular names to keep\n",
    "names_to_keep = ['Healthy Control', \"Parkinson's Disease\"]\n",
    "\n",
    "\n",
    "# Filter the dataframe based on the specified names\n",
    "PK_HC_pateints = patinets_diagnosis[patinets_diagnosis['COHORT_DEFINITION'].isin(names_to_keep)]\n",
    "\n",
    "# Get the list of patient IDs with diagnosis from the second dataframe\n",
    "patient_ids_with_diagnosis = PK_HC_pateints['PATNO']\n",
    "list_patients=list(patient_ids_with_diagnosis)\n",
    "\n",
    "# Filter the columns in the first dataframe based on patient IDs with diagnosis\n",
    "rna_filtered = filtered_read_ir3_counts.filter(items=list_patients)\n",
    "\n",
    "# We read a file that contains the Patient IDs that they were taking dopomine drugs, so they needed to be excluded.\n",
    "patient_dopomine = pd.read_csv(path2/'Patient_IDs_taking_dopamine_drugs.txt',delimiter='\\t',  header=None)\n",
    "patient_dopomine = patient_dopomine.rename(columns={0: 'Pateint IDs'})\n",
    "ids_to_remove = patient_dopomine['Pateint IDs'].tolist() # put the patient IDs to list\n",
    "strings = [str(num) for num in ids_to_remove] # convert them as string\n",
    "\n",
    "# The code is iterating over each column name in rna.columns and checking if any of the strings in the strings list \n",
    "# are present in that column name. If none of the strings are found in the column name,\n",
    "# then that column name is added to the new_columns list.\n",
    "new_columns = [col for col in rna_filtered.columns if not any(string in col for string in strings)] \n",
    "rna_filtered = rna_filtered[new_columns]\n",
    "# there were no column name (patints that use druf in this list) to be excluded in our case.\n",
    "# IN CASE THERE WERE SOME PATIENTS TO BE REMOVED, the diagnosis file below needs to be amended too.\n",
    "\n",
    "rna_filtered.to_csv(path2/'ir3_rna_step1.csv', index=True)\n",
    "\n",
    "# we keep only the patients that are common in the two dataframes:\n",
    "common_patient_ids = list(set(PK_HC_pateints['PATNO']).intersection(rna_filtered.columns))\n",
    "patient11_filtered = PK_HC_pateints[PK_HC_pateints['PATNO'].isin(common_patient_ids)]\n",
    "patient11_filtered.reset_index(drop=True)\n",
    "\n",
    "# we save the output into data folder\n",
    "patient11_filtered.to_csv(path2/'patients_HC_PK_diagnosis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f23fb",
   "metadata": {},
   "source": [
    "<a id=\"preprocessin2\"></a>\n",
    "## Data Preprocessing STEP II\n",
    "\n",
    "1. Removing lowely expressed genes, by keeping only genes that had more than five counts in at least 10% of the individuals, which left us with 25317 genes\n",
    "\n",
    "2. Similar DESeq2: we estimated size factors, normalized the library size bias using these factors, performed independent filtering to remove lowly expressed genes using the mean of normalized counts as a filter statistic. This left us with 22969 genes\n",
    "\n",
    "3. DESeq2: we apply a variance stabilizing transformation to accommodate the problem of unequal variance across the range of mean values.\n",
    "\n",
    "4. limma: we used control samples to estimate the batch effect of the site, that we subsequently removed in both controls and cases \n",
    "\n",
    "5. limma: we removed further confounding effects due to sex and RIN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643dcd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_step1 = pd.read_csv(path2/'ir3_rna_step1.csv')\n",
    "rna_step1.set_index('Geneid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb2a87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Removing lowely expressed genes, by keeping only genes that had more than five counts in \n",
    "#at least 10% of the individuals, which left us with 25317 genes\n",
    "gene_counts = rna_step1.sum(axis=1)\n",
    "gene_mask = gene_counts > 5\n",
    "gene_percentage = (rna_step1 > 5).mean(axis=1)\n",
    "percentage_mask = gene_percentage >= 0.1\n",
    "filtered_data = rna_step1[gene_mask & percentage_mask]\n",
    "\n",
    "# we estimated size factors, normalized the library size bias using these factors,\n",
    "# performed independent filtering to remove lowly expressed genes using the mean of normalized counts as a filter statistic.\n",
    "#This left us with 22969 genes\n",
    "# Step 1: Estimating Size Factors\n",
    "library_sizes = filtered_data.sum(axis=0)\n",
    "median_library_size = np.median(library_sizes)\n",
    "size_factors = library_sizes / median_library_size\n",
    "\n",
    "# Step 2: Normalizing Library Size Bias\n",
    "normalized_data = filtered_data.divide(size_factors, axis=1)\n",
    "\n",
    "# Step 3: Performing Independent Filtering\n",
    "mean_normalized_counts = normalized_data.mean(axis=1)\n",
    "threshold = 5  # Adjust this threshold as desired\n",
    "filtered_data2 = normalized_data.loc[mean_normalized_counts >= threshold]\n",
    "\n",
    "\n",
    "#we need to round and make the counts values integer because that what deseq2 type requires.\n",
    "filtered_data2 = filtered_data2.round().astype(int)\n",
    "filtered_data2 = filtered_data2.T\n",
    "# we make the patient ids as string type otherwise we get warning when transforming to deseq data set.\n",
    "filtered_data2.index = filtered_data2.index.astype(str)\n",
    "filtered_data2.to_csv(path2/'ir3_rna_step2.csv', index=True)\n",
    "\n",
    "\n",
    "diagnosis = pd.read_csv(path2/'patients_HC_PK_diagnosis.csv')\n",
    "patnn=diagnosis.set_index(\"PATNO\")\n",
    "# renaming the column as \"condition\" is necessary for deseq transformation.\n",
    "patnn.rename(columns={'COHORT_DEFINITION': 'condition'}, inplace=True)\n",
    "patnn.index = patnn.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75f0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is to make a dese data set:\n",
    "dds = DeseqDataSet(\n",
    "    counts=filtered_data2,\n",
    "    clinical=patnn,\n",
    "    design_factors=\"condition\"\n",
    ")\n",
    "#dds.obs # show patients diagnosis\n",
    "#dds.X # show array of counts\n",
    "# dds.var # show Geneids\n",
    "\n",
    "# Perform VST transformation\n",
    "dds.vst()\n",
    "\n",
    "# Here we get the VST data which are in the numpy form.\n",
    "vst_transformed_dds=dds.layers[\"vst_counts\"]\n",
    "\n",
    "# We convert the numpy data to pandas dataframe\n",
    "pd_vst= pd.DataFrame(vst_transformed_dds)\n",
    "\n",
    "# the above file does not have patient IDs name as well as Gene IDs so we need to take it from the other\n",
    "# file and then add it to bare dataframe file\n",
    "\n",
    "ir3_rna_step2 = pd.read_csv(path2/'ir3_rna_step2.csv')\n",
    "# patient IDs \n",
    "patient_ids = ir3_rna_step2['Unnamed: 0']\n",
    "\n",
    "# set them as the index of rows: \n",
    "pd_vst.set_index(patient_ids, inplace = True)\n",
    "\n",
    "# taking the gene IDs properly as a list format\n",
    "geneids = list(ir3_rna_step2.columns)[1:]\n",
    "\n",
    "# add the list of columns to the pandas dataframe file:\n",
    "pd_vst.columns = geneids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3bc62193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000000003</th>\n",
       "      <th>ENSG00000000419</th>\n",
       "      <th>ENSG00000000457</th>\n",
       "      <th>ENSG00000000460</th>\n",
       "      <th>ENSG00000000938</th>\n",
       "      <th>ENSG00000000971</th>\n",
       "      <th>ENSG00000001036</th>\n",
       "      <th>ENSG00000001084</th>\n",
       "      <th>ENSG00000001167</th>\n",
       "      <th>ENSG00000001460</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000285367</th>\n",
       "      <th>ENSG00000285373</th>\n",
       "      <th>ENSG00000285374</th>\n",
       "      <th>ENSG00000285399</th>\n",
       "      <th>ENSG00000285407</th>\n",
       "      <th>ENSG00000285410</th>\n",
       "      <th>ENSG00000285424</th>\n",
       "      <th>ENSG00000285448</th>\n",
       "      <th>ENSG00000285480</th>\n",
       "      <th>ENSG00000285486</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>7.444539</td>\n",
       "      <td>9.354533</td>\n",
       "      <td>10.785976</td>\n",
       "      <td>9.254673</td>\n",
       "      <td>13.236171</td>\n",
       "      <td>8.055200</td>\n",
       "      <td>9.156893</td>\n",
       "      <td>10.203048</td>\n",
       "      <td>11.013048</td>\n",
       "      <td>8.043249</td>\n",
       "      <td>...</td>\n",
       "      <td>7.467016</td>\n",
       "      <td>7.530827</td>\n",
       "      <td>7.026795</td>\n",
       "      <td>10.132249</td>\n",
       "      <td>7.071455</td>\n",
       "      <td>10.380051</td>\n",
       "      <td>7.112112</td>\n",
       "      <td>7.184684</td>\n",
       "      <td>7.347124</td>\n",
       "      <td>7.217629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>7.155375</td>\n",
       "      <td>10.134700</td>\n",
       "      <td>10.913692</td>\n",
       "      <td>9.232184</td>\n",
       "      <td>13.423261</td>\n",
       "      <td>8.769711</td>\n",
       "      <td>9.370862</td>\n",
       "      <td>10.426515</td>\n",
       "      <td>11.162764</td>\n",
       "      <td>7.745575</td>\n",
       "      <td>...</td>\n",
       "      <td>6.600057</td>\n",
       "      <td>7.539564</td>\n",
       "      <td>6.600057</td>\n",
       "      <td>10.047410</td>\n",
       "      <td>6.600057</td>\n",
       "      <td>10.750447</td>\n",
       "      <td>6.749323</td>\n",
       "      <td>7.155375</td>\n",
       "      <td>7.293787</td>\n",
       "      <td>7.211192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>7.970414</td>\n",
       "      <td>10.156059</td>\n",
       "      <td>10.773783</td>\n",
       "      <td>9.408801</td>\n",
       "      <td>13.153259</td>\n",
       "      <td>8.352062</td>\n",
       "      <td>9.630833</td>\n",
       "      <td>10.108905</td>\n",
       "      <td>10.909195</td>\n",
       "      <td>8.238319</td>\n",
       "      <td>...</td>\n",
       "      <td>8.078524</td>\n",
       "      <td>7.523101</td>\n",
       "      <td>6.915918</td>\n",
       "      <td>9.817891</td>\n",
       "      <td>7.088036</td>\n",
       "      <td>10.540444</td>\n",
       "      <td>7.340850</td>\n",
       "      <td>6.999118</td>\n",
       "      <td>7.327780</td>\n",
       "      <td>7.228072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>7.101245</td>\n",
       "      <td>10.156737</td>\n",
       "      <td>10.943662</td>\n",
       "      <td>9.402584</td>\n",
       "      <td>12.990781</td>\n",
       "      <td>9.168530</td>\n",
       "      <td>9.471058</td>\n",
       "      <td>11.208888</td>\n",
       "      <td>11.154034</td>\n",
       "      <td>8.001086</td>\n",
       "      <td>...</td>\n",
       "      <td>7.053809</td>\n",
       "      <td>7.459737</td>\n",
       "      <td>6.751862</td>\n",
       "      <td>9.889467</td>\n",
       "      <td>6.814643</td>\n",
       "      <td>10.440017</td>\n",
       "      <td>6.971050</td>\n",
       "      <td>6.903249</td>\n",
       "      <td>7.256390</td>\n",
       "      <td>7.520583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>7.285480</td>\n",
       "      <td>10.171662</td>\n",
       "      <td>11.068792</td>\n",
       "      <td>9.344570</td>\n",
       "      <td>13.816320</td>\n",
       "      <td>9.170579</td>\n",
       "      <td>9.367399</td>\n",
       "      <td>9.995035</td>\n",
       "      <td>11.226984</td>\n",
       "      <td>8.099689</td>\n",
       "      <td>...</td>\n",
       "      <td>7.749501</td>\n",
       "      <td>7.621640</td>\n",
       "      <td>6.960446</td>\n",
       "      <td>10.281056</td>\n",
       "      <td>6.894567</td>\n",
       "      <td>10.586627</td>\n",
       "      <td>7.270002</td>\n",
       "      <td>7.040873</td>\n",
       "      <td>7.237837</td>\n",
       "      <td>7.064519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>7.457444</td>\n",
       "      <td>10.225018</td>\n",
       "      <td>10.895034</td>\n",
       "      <td>9.438303</td>\n",
       "      <td>13.354071</td>\n",
       "      <td>9.097724</td>\n",
       "      <td>9.433466</td>\n",
       "      <td>11.155257</td>\n",
       "      <td>11.290757</td>\n",
       "      <td>7.749228</td>\n",
       "      <td>...</td>\n",
       "      <td>6.948633</td>\n",
       "      <td>7.604369</td>\n",
       "      <td>6.911984</td>\n",
       "      <td>9.948419</td>\n",
       "      <td>6.820837</td>\n",
       "      <td>10.525281</td>\n",
       "      <td>6.870325</td>\n",
       "      <td>7.159967</td>\n",
       "      <td>7.180827</td>\n",
       "      <td>7.239127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135</th>\n",
       "      <td>7.110766</td>\n",
       "      <td>9.602013</td>\n",
       "      <td>10.746912</td>\n",
       "      <td>9.474189</td>\n",
       "      <td>13.105243</td>\n",
       "      <td>9.543963</td>\n",
       "      <td>9.641465</td>\n",
       "      <td>12.454553</td>\n",
       "      <td>11.151544</td>\n",
       "      <td>7.967019</td>\n",
       "      <td>...</td>\n",
       "      <td>6.945353</td>\n",
       "      <td>7.719699</td>\n",
       "      <td>6.978130</td>\n",
       "      <td>10.410234</td>\n",
       "      <td>6.754773</td>\n",
       "      <td>10.394679</td>\n",
       "      <td>6.818753</td>\n",
       "      <td>6.909046</td>\n",
       "      <td>7.302436</td>\n",
       "      <td>7.475752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>7.189417</td>\n",
       "      <td>9.248525</td>\n",
       "      <td>10.266971</td>\n",
       "      <td>9.027006</td>\n",
       "      <td>13.772930</td>\n",
       "      <td>8.524958</td>\n",
       "      <td>9.038026</td>\n",
       "      <td>11.022504</td>\n",
       "      <td>10.773990</td>\n",
       "      <td>7.979105</td>\n",
       "      <td>...</td>\n",
       "      <td>7.722990</td>\n",
       "      <td>7.326301</td>\n",
       "      <td>6.849054</td>\n",
       "      <td>9.988918</td>\n",
       "      <td>6.921246</td>\n",
       "      <td>10.283935</td>\n",
       "      <td>7.189417</td>\n",
       "      <td>7.075290</td>\n",
       "      <td>7.312480</td>\n",
       "      <td>7.722990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>7.233663</td>\n",
       "      <td>10.159731</td>\n",
       "      <td>10.932466</td>\n",
       "      <td>9.094123</td>\n",
       "      <td>13.501251</td>\n",
       "      <td>8.377765</td>\n",
       "      <td>9.794411</td>\n",
       "      <td>10.466412</td>\n",
       "      <td>10.984662</td>\n",
       "      <td>8.010543</td>\n",
       "      <td>...</td>\n",
       "      <td>7.299313</td>\n",
       "      <td>7.358913</td>\n",
       "      <td>6.812768</td>\n",
       "      <td>9.717669</td>\n",
       "      <td>6.997109</td>\n",
       "      <td>10.395415</td>\n",
       "      <td>6.997109</td>\n",
       "      <td>7.216081</td>\n",
       "      <td>7.118764</td>\n",
       "      <td>7.546932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41410</th>\n",
       "      <td>7.134216</td>\n",
       "      <td>9.587791</td>\n",
       "      <td>10.708402</td>\n",
       "      <td>9.370643</td>\n",
       "      <td>13.691602</td>\n",
       "      <td>8.438516</td>\n",
       "      <td>9.306976</td>\n",
       "      <td>10.779137</td>\n",
       "      <td>11.016545</td>\n",
       "      <td>7.881798</td>\n",
       "      <td>...</td>\n",
       "      <td>6.748988</td>\n",
       "      <td>7.492092</td>\n",
       "      <td>6.857784</td>\n",
       "      <td>10.224040</td>\n",
       "      <td>6.600057</td>\n",
       "      <td>10.495499</td>\n",
       "      <td>6.600057</td>\n",
       "      <td>7.020000</td>\n",
       "      <td>7.244156</td>\n",
       "      <td>7.227246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows × 22969 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ENSG00000000003  ENSG00000000419  ENSG00000000457  \\\n",
       "Unnamed: 0                                                      \n",
       "3000               7.444539         9.354533        10.785976   \n",
       "3001               7.155375        10.134700        10.913692   \n",
       "3002               7.970414        10.156059        10.773783   \n",
       "3003               7.101245        10.156737        10.943662   \n",
       "3004               7.285480        10.171662        11.068792   \n",
       "...                     ...              ...              ...   \n",
       "4126               7.457444        10.225018        10.895034   \n",
       "4135               7.110766         9.602013        10.746912   \n",
       "4136               7.189417         9.248525        10.266971   \n",
       "4139               7.233663        10.159731        10.932466   \n",
       "41410              7.134216         9.587791        10.708402   \n",
       "\n",
       "            ENSG00000000460  ENSG00000000938  ENSG00000000971  \\\n",
       "Unnamed: 0                                                      \n",
       "3000               9.254673        13.236171         8.055200   \n",
       "3001               9.232184        13.423261         8.769711   \n",
       "3002               9.408801        13.153259         8.352062   \n",
       "3003               9.402584        12.990781         9.168530   \n",
       "3004               9.344570        13.816320         9.170579   \n",
       "...                     ...              ...              ...   \n",
       "4126               9.438303        13.354071         9.097724   \n",
       "4135               9.474189        13.105243         9.543963   \n",
       "4136               9.027006        13.772930         8.524958   \n",
       "4139               9.094123        13.501251         8.377765   \n",
       "41410              9.370643        13.691602         8.438516   \n",
       "\n",
       "            ENSG00000001036  ENSG00000001084  ENSG00000001167  \\\n",
       "Unnamed: 0                                                      \n",
       "3000               9.156893        10.203048        11.013048   \n",
       "3001               9.370862        10.426515        11.162764   \n",
       "3002               9.630833        10.108905        10.909195   \n",
       "3003               9.471058        11.208888        11.154034   \n",
       "3004               9.367399         9.995035        11.226984   \n",
       "...                     ...              ...              ...   \n",
       "4126               9.433466        11.155257        11.290757   \n",
       "4135               9.641465        12.454553        11.151544   \n",
       "4136               9.038026        11.022504        10.773990   \n",
       "4139               9.794411        10.466412        10.984662   \n",
       "41410              9.306976        10.779137        11.016545   \n",
       "\n",
       "            ENSG00000001460  ...  ENSG00000285367  ENSG00000285373  \\\n",
       "Unnamed: 0                   ...                                     \n",
       "3000               8.043249  ...         7.467016         7.530827   \n",
       "3001               7.745575  ...         6.600057         7.539564   \n",
       "3002               8.238319  ...         8.078524         7.523101   \n",
       "3003               8.001086  ...         7.053809         7.459737   \n",
       "3004               8.099689  ...         7.749501         7.621640   \n",
       "...                     ...  ...              ...              ...   \n",
       "4126               7.749228  ...         6.948633         7.604369   \n",
       "4135               7.967019  ...         6.945353         7.719699   \n",
       "4136               7.979105  ...         7.722990         7.326301   \n",
       "4139               8.010543  ...         7.299313         7.358913   \n",
       "41410              7.881798  ...         6.748988         7.492092   \n",
       "\n",
       "            ENSG00000285374  ENSG00000285399  ENSG00000285407  \\\n",
       "Unnamed: 0                                                      \n",
       "3000               7.026795        10.132249         7.071455   \n",
       "3001               6.600057        10.047410         6.600057   \n",
       "3002               6.915918         9.817891         7.088036   \n",
       "3003               6.751862         9.889467         6.814643   \n",
       "3004               6.960446        10.281056         6.894567   \n",
       "...                     ...              ...              ...   \n",
       "4126               6.911984         9.948419         6.820837   \n",
       "4135               6.978130        10.410234         6.754773   \n",
       "4136               6.849054         9.988918         6.921246   \n",
       "4139               6.812768         9.717669         6.997109   \n",
       "41410              6.857784        10.224040         6.600057   \n",
       "\n",
       "            ENSG00000285410  ENSG00000285424  ENSG00000285448  \\\n",
       "Unnamed: 0                                                      \n",
       "3000              10.380051         7.112112         7.184684   \n",
       "3001              10.750447         6.749323         7.155375   \n",
       "3002              10.540444         7.340850         6.999118   \n",
       "3003              10.440017         6.971050         6.903249   \n",
       "3004              10.586627         7.270002         7.040873   \n",
       "...                     ...              ...              ...   \n",
       "4126              10.525281         6.870325         7.159967   \n",
       "4135              10.394679         6.818753         6.909046   \n",
       "4136              10.283935         7.189417         7.075290   \n",
       "4139              10.395415         6.997109         7.216081   \n",
       "41410             10.495499         6.600057         7.020000   \n",
       "\n",
       "            ENSG00000285480  ENSG00000285486  \n",
       "Unnamed: 0                                    \n",
       "3000               7.347124         7.217629  \n",
       "3001               7.293787         7.211192  \n",
       "3002               7.327780         7.228072  \n",
       "3003               7.256390         7.520583  \n",
       "3004               7.237837         7.064519  \n",
       "...                     ...              ...  \n",
       "4126               7.180827         7.239127  \n",
       "4135               7.302436         7.475752  \n",
       "4136               7.312480         7.722990  \n",
       "4139               7.118764         7.546932  \n",
       "41410              7.244156         7.227246  \n",
       "\n",
       "[583 rows x 22969 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_vst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d6f91",
   "metadata": {},
   "source": [
    "# RIN and SEX effects to be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4df503",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_rin_data = pd.read_csv(path2/'Patient_IDs_RNA_sample_RIN_sex_CNO_diagnosis.txt',delimiter='\\t')\n",
    "sex_rin_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = sex_rin_data['CLINICAL_EVENT'].unique()\n",
    "unique_values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf29734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to keep only base line visit data of patient.\n",
    "#baseline_df = sex_rin_data[sex_rin_data['CLINICAL_EVENT'] == 'BL']\n",
    "# Filter the DataFrame to keep rows with 'BL' and 'SC' values in the 'CLINICAL_EVENT' column\n",
    "baseline_df = sex_rin_data[sex_rin_data['CLINICAL_EVENT'].isin(['BL'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis = pd.read_csv(path2/'patients_HC_PK_diagnosis.csv')\n",
    "diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = diagnosis['PATNO']\n",
    "filtered_df_sex_rin = baseline_df[baseline_df['ALIAS_ID'].isin(patient_ids)]\n",
    "filtered_df_sex_rin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all patient IDs in diagnosis exist in filtered_df_sex_rin\n",
    "all_exist = diagnosis['PATNO'].isin(baseline_df['ALIAS_ID']).all()\n",
    "\n",
    "# Print the result\n",
    "if all_exist:\n",
    "    print(\"All patient IDs in other_df exist in big_df.\")\n",
    "else:\n",
    "    print(\"Not all patient IDs in other_df exist in big_df.\")\n",
    "# I note that all the patient IDs that I analysis are note only in BL some of them are in V01.\n",
    "# I need to find those that are in the v01 and only add them not the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679dba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = filtered_df_sex_rin['ALIAS_ID'].duplicated()\n",
    "duplicate_rows = filtered_df_sex_rin[duplicates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d01204",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicated patient IDs in the filtered DataFrame\n",
    "duplicates = filtered_df_sex_rin['ALIAS_ID'].duplicated(keep=False)\n",
    "\n",
    "# Filter the DataFrame to keep only the duplicated rows\n",
    "duplicate_rows = filtered_df_sex_rin[duplicates]\n",
    "\n",
    "# Sort the duplicate rows by the patient ID\n",
    "duplicate_rows_sorted = duplicate_rows.sort_values('ALIAS_ID')\n",
    "\n",
    "# Print the duplicate rows\n",
    "duplicate_rows_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5d06c",
   "metadata": {},
   "source": [
    "<a id=\"training\"></a>\n",
    "## Model Training \n",
    "\n",
    "Build and train machine learning models on the prepared data. Explain the choice of models, feature engineering techniques, and hyperparameter tuning. Provide code and comments to walk through the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6376df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = pd.read_csv(path2/'ir3_rna.csv')\n",
    "rna.set_index('Geneid', inplace=True)\n",
    "diagnosis = pd.read_csv(path2/'patients_HC_PK_diagnosis.csv')\n",
    "\n",
    "# transposing the matrix, so the patient IDs would be the first column\n",
    "rna_ir3 = rna.T\n",
    "\n",
    "# mapping diagnosis to zero and one.\n",
    "diagnosis['COHORT_DEFINITION'] = diagnosis['COHORT_DEFINITION'].map({'Healthy Control': 0, \"Parkinson's Disease\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_ir3.rename_axis(\"PATNO\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see how many Parkinson and Healthy patients we have\n",
    "diagnosis['COHORT_DEFINITION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation metric lists\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Perform the iterations\n",
    "for _ in range(20):\n",
    "    # Split the data into training and test sets\n",
    "    # X_train: train data of RNA,  X_test: test data of RNA \n",
    "    # y_train: train diagnosis, y_test: test diagnosis\n",
    "    X_train, X_test, y_train, y_test = train_test_split(rna_ir3, diagnosis['COHORT_DEFINITION'], test_size=.3)\n",
    "    \n",
    "    # Create and fit the Random Forest model\n",
    "    model_rf = RandomForestClassifier(n_estimators=100)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model and store the metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, model_rf.predict(X_test)))\n",
    "    precision_scores.append(precision_score(y_test, model_rf.predict(X_test)))\n",
    "    recall_scores.append(recall_score(y_test, model_rf.predict(X_test)))\n",
    "    f1_scores.append(f1_score(y_test, model_rf.predict(X_test)))\n",
    "    roc_auc_scores.append(roc_auc_score(y_test, model_rf.predict(X_test)))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, model_rf.predict(X_test)))\n",
    "\n",
    "# Calculate the average evaluation metrics\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_roc_auc = np.mean(roc_auc_scores)\n",
    "avg_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "# Print the average evaluation metrics\n",
    "print('Average Evaluation Metrics')\n",
    "print('Accuracy:  ', avg_accuracy)\n",
    "print('Precision: ', avg_precision)\n",
    "print('Recall:    ', avg_recall)\n",
    "print('F1:        ', avg_f1)\n",
    "print('ROC-AUC:   ', avg_roc_auc)\n",
    "print('\\nAverage Confusion Matrix\\n', avg_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Assuming you have your RNA-Seq data in X and corresponding labels in y\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Define the repeated stratified cross-validation strategy\n",
    "cross_val = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n",
    "\n",
    "# Perform repeated stratified cross-validation\n",
    "permutation_importances = []\n",
    "for _ in range(100):\n",
    "    # Train and evaluate the random forest model\n",
    "    for train_index, test_index in cross_val.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Calculate permutation importances\n",
    "        result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "        permutation_importances.append(result.importances_mean)\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "permutation_importances = np.array(permutation_importances)\n",
    "\n",
    "# Print the average permutation importances across all repetitions\n",
    "print(\"Average permutation importances:\")\n",
    "print(np.mean(permutation_importances, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a54690",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27602ca3",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## Results and Evaluation \n",
    "\n",
    "Present the results of the trained models, including performance metrics, accuracy, or any relevant evaluation measures. Interpret the findings and discuss the implications. Include visualizations or tables to support the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "# Initialize lists to store the ROC and precision-recall data\n",
    "roc_curves = []\n",
    "precision_recall_curves = []\n",
    "\n",
    "# Perform the iterations\n",
    "for _ in range(20):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(rna_ir3, diagnosis['COHORT_DEFINITION'], test_size=.3)\n",
    "    \n",
    "    # Create and fit the Random Forest model\n",
    "    model_rf = RandomForestClassifier(n_estimators=100)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
    "    roc_curves.append((fpr, tpr))\n",
    "    \n",
    "    # Calculate the precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
    "    precision_recall_curves.append((precision, recall))\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for fpr, tpr in roc_curves:\n",
    "    plt.plot(fpr, tpr, lw=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for precision, recall in precision_recall_curves:\n",
    "    plt.plot(recall, precision, lw=1)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60138fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting roc curve and precision recall curve\n",
    "roc = roc_curve(y_test,model_rf.predict_proba(X_test)[:,1])\n",
    "pr  = precision_recall_curve(y_test,model_rf.predict_proba(X_test)[:,1])\n",
    "\n",
    "f = plt.figure(figsize=(20,7))\n",
    "ax = f.add_subplot(121)\n",
    "ax.plot(roc[0],roc[1])\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.grid(which='both')\n",
    "ax = f.add_subplot(122)\n",
    "ax.plot(pr[1],pr[0])\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_title('PR-curve')\n",
    "ax.grid(which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f773451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
