{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d184cd",
   "metadata": {},
   "source": [
    "# <span style=\"color:#8B4513;\"> Machine Learning and RNA-Seq Data of Parkinson Disease\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "[<span style=\"color:#8B4513;\">Author: **Zainab Nazari**</span>](mailto:z.nazari@ebri.com)\n",
    " \n",
    " <span style=\"color:#8B4513;\">EBRI – European Brain Research Institute Rita Levi-Montalcini | MHPC - Master in High Performance Computing</span>\n",
    " \n",
    "\n",
    "\n",
    "## Introduction\n",
    "By employing machine learning in PPMI clinical data set, we can develop predictive models that aid in the early diagnosis of the disease. These models can potentially identify specific genetic markers or gene signatures that correlate with disease progression or response to treatment.\n",
    "\n",
    "## Table of Contents\n",
    "- [Matrix of Gene IDs and Counts for Pateints](#matrixcreation)\n",
    "- [Data Preprocessing STEP I](#preprocessing)\n",
    "- [Data Preprocessing STEP II](#preprocessing2)\n",
    "- [Model Training](#training)\n",
    "- [Results and Evaluation](#results)\n",
    "\n",
    "## Matrix of Gene IDs and Counts for Patients\n",
    "- Loading the data from IR3/counts folder and extracting the associated last column (counts) of each patient file for their BL visit.\n",
    "\n",
    "\n",
    "## Data Preprocessing STEP I\n",
    "- We remove patients that have these diseases: SNCA (ENRLSNCA), GBA (ENRLGBA), LRRK2 (ENRLLRRK2).\n",
    "-  We only keep genes with the intersection of counts and quants with proteing coding and RNAincs.\n",
    "- We remove the duplicated gene IDs in which they are also lowly expressed.\n",
    "- We keep only patients with diagnosis of Health control or Parkinson disease.\n",
    "- We check if there are some patients were they were taking dopamine drug, so we exclude them. Dopaminergic medication can impact the interpretation of experimental data or measurements and can alter gene expression patterns, so we need to remove them to have less biased data.\n",
    "\n",
    "## Data Preprocessing STEP II\n",
    "1. We remove lowely expressed genes, by keeping only genes that had more than five counts in at least 10% of the individuals, which left us with 21,273 genes\n",
    "\n",
    "2. Similar DESeq2 but with numpy:  we estimated size factors, normalized the library size bias using these factors, performed independent filtering to remove lowly expressed genes using the mean of normalized counts as a filter statistic. This left us with 22969 genes\n",
    "\n",
    "3. pyDESeq2: we apply a variance stabilizing transformation (vst) to accommodate the problem of unequal variance across the range of mean values.\n",
    "\n",
    "\n",
    "4. limma: we used control samples to estimate the batch effect of the site, that we subsequently removed in both controls and cases. In experimental research, a batch effect is a systematic variation in data that can occur when data is collected from multiple sites (clinical centers). These factors can include differences in equipment, reagents, operators, or experimental conditions. Examples of batch effects: \n",
    " - Differences in the equipment used to collect the data. For example, if you are using two different microarray platforms to measure gene expression, there may be differences in the way that the platforms detect and quantify gene expression.\n",
    " - Differences in the operators who collect the data. For example, if two different people are collecting RNA-seq data, they may have different levels of experience or expertise, which could lead to differences in the way that they process the samples.\n",
    " \n",
    "\n",
    "5. using limma: we removed further confounding effects due to sex and RIN value. RIN value is a measure of the quality of RNA samples, and it can vary depending on the sample preparation method. Sex can also affect gene expression. If the effects of sex and RIN value are not removed, then the results of the analysis may be biased.\n",
    "\n",
    "\n",
    "## Model Training\n",
    "The code uses a Random Forest model to identify the most important features in a dataset. The code first performs\n",
    "repeated stratified k-fold cross validation to train the Random Forest and compute the permutaion featute importanes. Then, the code counts the occurances of each features in the selected top features ist. Finally, the code gets the name of the final selected top features.\n",
    "\n",
    "## Results and Evaluation\n",
    "We present the results of the trained models, including performance metrics, accuracy, or any relevant evaluation measures. The model without preprocessing is with high recall score and low roc and auc score, and this means that the model is good to distinguishing the person with parkinson but not healthy people, therefore the model sounds very random.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "Summarize the key findings, limitations of the analysis, and potential future work or improvements. Offer closing remarks or suggestions for further exploration.\n",
    "\n",
    "## References\n",
    "- [**Parkinson’s Progression Markers Initiative (PPMI)**](https://www.ppmi-info.org/)\n",
    "\n",
    "- [**A Machine Learning Approach to Parkinson’s Disease Blood Transcriptomics**](https://www.mdpi.com/2073-4425/13/5/727)\n",
    "\n",
    "- [**Quality Control Metrics for Whole Blood Transcriptome Analysis in the Parkinson’s Progression Markers Initiative (PPMI)**](https://www.medrxiv.org/content/10.1101/2021.01.05.21249278v1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7b8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you do not have following packages installed, uncomment instalisation.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip install dask[complete];\n",
    "# you need to run these in case dask gives you error, it might need update.\n",
    "#!pip install --upgrade pandas \"dask[complete]\"\n",
    "#python -m pip install \"dask[dataframe]\" --upgrade\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance       \n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#!pip3 install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#!pip install conorm\n",
    "import conorm # for tmm normalisation\n",
    "\n",
    "#!pip3 install pydeseq2 or pip install pydeseq2\n",
    "from pydeseq2.dds import DeseqDataSet\n",
    "from pydeseq2.ds import DeseqStats\n",
    "from pydeseq2.utils import load_example_data\n",
    "\n",
    "\n",
    "\n",
    "#to install R :\n",
    "#conda install -c r r-irkernel\n",
    "\n",
    "#to install a library from R\n",
    "#!pip install library edgeR\n",
    "# pip install rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba5df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the counts file in the IR3 is around 152 G, and the files are located in scratch area.\n",
    "\n",
    "path_to_files=\"/scratch/znazari/PPMI_ver_sep2022/RNA_Seq_data/star_ir3/counts/\"\n",
    "path1=Path(\"/scratch/znazari/PPMI_ver_sep2022/RNA_Seq_data/star_ir3/counts/\")\n",
    "path2 = Path(\"/home/znazari/data\") # where the output data will be saved at the end.\n",
    "path3=Path(\"/scratch/znazari/PPMI_ver_sep2022/study_data/Subject_Characteristics/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86494e1a",
   "metadata": {},
   "source": [
    "<a id=\"matrixcreation\"></a>\n",
    "## Matrix of Gene IDs and Counts for Patients\n",
    " Loading the data from IR3/counts folder and extracting the associated last column (counts) of each patient file for their BL visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b8db8",
   "metadata": {
    "tags": [
     "matrix-creation"
    ]
   },
   "outputs": [],
   "source": [
    "#reading the files which are in BL (Base line) visit.\n",
    "specific_word = 'BL'\n",
    "ending_pattern = '*.txt'\n",
    "file_pattern = f'*{specific_word}*.{ending_pattern}'\n",
    "file_paths = glob.glob(path_to_files + file_pattern)\n",
    "# 'bl.txt' is a file that ccontains the name of the files with patient, BL, IR3, counts.\n",
    "filename = 'bl.txt'\n",
    "file_path_2 = os.path.join(path_to_files, filename)\n",
    "bl_files = pd.read_csv(file_path_2,header=None)\n",
    "\n",
    "# We define a function where we can take the second phrase seperated by dot. The second phrase \n",
    "# is the patient ID. So with this functin we want to get the patient IDs from their file's name\n",
    "def function_names(fname):\n",
    "    tokens=fname.split('.')\n",
    "    return tokens[1]\n",
    "\n",
    "# we create a list with the name of the each patients.\n",
    "bl_list = [function_names(bl_files.iloc[i][0]) for i in range(len(bl_files))]\n",
    "\n",
    "# here we read all the files with with base visit(BL) from the counts folder (where we have all the files\n",
    "# for all the patients and all the visit).\n",
    "list_bl_files = [dd.read_csv(path1/bl_files.iloc[i][0],skiprows=1,delimiter='\\t') for i in range(len(bl_files))]\n",
    "\n",
    "\n",
    "# we get th last columns of each file in the list\n",
    "last_columns = [ddf.iloc[:, -1:] for ddf in list_bl_files]\n",
    "\n",
    "# concatinating the list of the columns in a single file.\n",
    "single_file = dd.concat(last_columns, axis=1)\n",
    "\n",
    "# we change the name of the each columns with the patient numbers.\n",
    "single_file.columns = bl_list\n",
    "\n",
    "# we get the Geneid column and convert it to dask dataframe\n",
    "pd_tmp_file = list_bl_files[3].compute()\n",
    "geneid = pd_tmp_file['Geneid']\n",
    "ddf_geneid = dd.from_pandas(geneid, npartitions=1)\n",
    "\n",
    "# here we set the Geneid column as the index of the matrix.\n",
    "ddf_new_index = single_file.set_index(ddf_geneid)\n",
    "\n",
    "# converting to pandas data frame and saving.\n",
    "ir3_counts = ddf_new_index.compute()\n",
    "ir3_counts.to_csv(path2/\"matrix_ir3_counts_bl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab10c6",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "## Data Preprocessing STEP I\n",
    "\n",
    "- We remove patients that have these gene mutations : SNCA (ENRLSNCA), GBA (ENRLGBA), LRRK2 (ENRLLRRK2).\n",
    "- dopamin drug using\n",
    "-  We only keep genes with the intersection of counts and quants with proteing coding and RNAincs.\n",
    "- We remove the duplicated gene IDs in which they are also lowly expressed.\n",
    "- We keep only patients with diagnosis of Health control or Parkinson disease.\n",
    "- We check if there are some patients were they were taking dopomine drug, so we exclude them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07749200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file\n",
    "read_ir3_counts = pd.read_csv(path2/\"matrix_ir3_counts_bl.csv\")\n",
    "# setting the geneid as indexing column\n",
    "read_ir3_counts.set_index('Geneid', inplace=True)\n",
    "# result with removing the after dot (.) value, i.e. the version of the geneIDs is removed.\n",
    "read_ir3_counts.index =read_ir3_counts.index.str.split('.').str[0]\n",
    "\n",
    "\n",
    "#here we delete the duplicated gene IDs, first we find them then remove them from the gene IDs\n",
    "# as they are duplicated and also they are very lowly expressed either zero or one in rare caes.\n",
    "\n",
    "# Check for duplicate index values\n",
    "is_duplicate = read_ir3_counts.index.duplicated()\n",
    "\n",
    "# Display the duplicate index values\n",
    "duplicate_indices = read_ir3_counts.index[is_duplicate]\n",
    "\n",
    "# drop them (duplicated indices and their copies are deleted, 45 duplicatd indices and 90 are dropped)\n",
    "to_be_deleted = list(duplicate_indices)\n",
    "read_ir3_counts = read_ir3_counts.drop(to_be_deleted)\n",
    "\n",
    "# we read the file where we have an intersection of geneIDs in IR3, counts, quant\n",
    "intersect = pd.read_csv(path2/\"intersect_IR3_ENG_IDs_LincRNA_ProtCoding_counts_quant_gene_transcript_only_tot_intsersect.txt\")\n",
    "intersection = read_ir3_counts.index.intersection(intersect['[IR3_gene_counts] and [IR3_quant_gene] and [IR3_quant_trans] and [lncRNA+ProtCod]: '])\n",
    "filtered_read_ir3_counts = read_ir3_counts.loc[intersection]\n",
    "\n",
    "# reading the file which contains diagnosis\n",
    "diago=pd.read_csv(path3/\"Participant_Status.csv\", header=None )\n",
    "diago1=diago.rename(columns=diago.iloc[0]).drop(diago.index[0]).reset_index(drop=True)\n",
    "\n",
    "#this is to remove patients that have these diseases: SNCA (ENRLSNCA), GBA (ENRLGBA), LRRK2 (ENRLLRRK2)\n",
    "filtered_SNCA_GBA_LRRK2 = diago1[(diago1['ENRLSNCA'] == \"0\")& (diago1['ENRLGBA'] == \"0\")& (diago1['ENRLLRRK2'] == \"0\")]\n",
    "\n",
    "#patients with their diagnosis\n",
    "patinets_diagnosis = filtered_SNCA_GBA_LRRK2[['PATNO','COHORT_DEFINITION']].reset_index(drop=True)\n",
    "\n",
    "# Define the particular names to keep\n",
    "names_to_keep = ['Healthy Control', \"Parkinson's Disease\"]\n",
    "\n",
    "\n",
    "# Filter the dataframe based on the specified names\n",
    "PK_HC_pateints = patinets_diagnosis[patinets_diagnosis['COHORT_DEFINITION'].isin(names_to_keep)]\n",
    "\n",
    "# Get the list of patient IDs with diagnosis from the second dataframe\n",
    "patient_ids_with_diagnosis = PK_HC_pateints['PATNO']\n",
    "list_patients=list(patient_ids_with_diagnosis)\n",
    "\n",
    "# Filter the columns in the first dataframe based on patient IDs with diagnosis\n",
    "rna_filtered = filtered_read_ir3_counts.filter(items=list_patients)\n",
    "\n",
    "# We read a file that contains the Patient IDs that they were taking dopomine drugs, so they needed to be excluded.\n",
    "patient_dopomine = pd.read_csv(path2/'Patient_IDs_taking_dopamine_drugs.txt',delimiter='\\t',  header=None)\n",
    "patient_dopomine = patient_dopomine.rename(columns={0: 'Pateint IDs'})\n",
    "ids_to_remove = patient_dopomine['Pateint IDs'].tolist() # put the patient IDs to list\n",
    "strings = [str(num) for num in ids_to_remove] # convert them as string\n",
    "\n",
    "# The code is iterating over each column name in rna.columns and checking if any of the strings in the strings list \n",
    "# are present in that column name. If none of the strings are found in the column name,\n",
    "# then that column name is added to the new_columns list.\n",
    "new_columns = [col for col in rna_filtered.columns if not any(string in col for string in strings)] \n",
    "rna_filtered = rna_filtered[new_columns]\n",
    "# there were no column name (patints that use druf in this list) to be excluded in our case.\n",
    "# IN CASE THERE WERE SOME PATIENTS TO BE REMOVED, the diagnosis file below needs to be amended too.\n",
    "\n",
    "rna_filtered.to_csv(path2/'ir3_rna_step1.csv', index=True)\n",
    "\n",
    "# we keep only the patients that are common in the two dataframes:\n",
    "common_patient_ids = list(set(PK_HC_pateints['PATNO']).intersection(rna_filtered.columns))\n",
    "patient11_filtered = PK_HC_pateints[PK_HC_pateints['PATNO'].isin(common_patient_ids)]\n",
    "patient11_filtered.reset_index(drop=True)\n",
    "\n",
    "# we save the output into data folder\n",
    "patient11_filtered.to_csv(path2/'patients_HC_PK_diagnosis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f23fb",
   "metadata": {},
   "source": [
    "<a id=\"preprocessin2\"></a>\n",
    "## Data Preprocessing STEP II\n",
    "\n",
    "1. Removing lowely expressed genes, by keeping only genes that had more than five counts in at least 10% of the individuals, which left us with 25317 genes\n",
    "\n",
    "2. Similar DESeq2: we estimated size factors, normalized the library size bias using these factors, performed independent filtering to remove lowly expressed genes using the mean of normalized counts as a filter statistic. This left us with 22969 genes\n",
    "\n",
    "3. DESeq2: we apply a variance stabilizing transformation to accommodate the problem of unequal variance across the range of mean values.\n",
    "\n",
    "4. limma: we used control samples to estimate the batch effect of the site, that we subsequently removed in both controls and cases \n",
    "\n",
    "5. limma: we removed further confounding effects due to sex and RIN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643dcd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_step1 = pd.read_csv(path2/'ir3_rna_step1.csv')\n",
    "rna_step1.set_index('Geneid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Removing lowely expressed genes, by keeping only genes that had more than five counts in \n",
    "#at least 10% of the individuals, which left us with 25317 genes\n",
    "gene_counts = rna_step1.sum(axis=1)\n",
    "gene_mask = gene_counts > 5\n",
    "gene_percentage = (rna_step1 > 5).mean(axis=1)\n",
    "percentage_mask = gene_percentage >= 0.1\n",
    "filtered_data = rna_step1[gene_mask & percentage_mask]\n",
    "\n",
    "# we estimated size factors, normalized the library size bias using these factors,\n",
    "# performed independent filtering to remove lowly expressed genes using the mean of normalized counts as a filter statistic.\n",
    "#This left us with 22969 genes\n",
    "# Step 1: Estimating Size Factors\n",
    "library_sizes = filtered_data.sum(axis=0)\n",
    "median_library_size = np.median(library_sizes)\n",
    "size_factors = library_sizes / median_library_size\n",
    "\n",
    "# Step 2: Normalizing Library Size Bias\n",
    "normalized_data = filtered_data.divide(size_factors, axis=1)\n",
    "\n",
    "# Step 3: Performing Independent Filtering\n",
    "mean_normalized_counts = normalized_data.mean(axis=1)\n",
    "threshold = 5  # Adjust this threshold as desired\n",
    "filtered_data2 = normalized_data.loc[mean_normalized_counts >= threshold]\n",
    "\n",
    "\n",
    "#we need to round and make the counts values integer because that what deseq2 type requires.\n",
    "filtered_data2 = filtered_data2.round().astype(int)\n",
    "filtered_data2 = filtered_data2.T\n",
    "# we make the patient ids as string type otherwise we get warning when transforming to deseq data set.\n",
    "filtered_data2.index = filtered_data2.index.astype(str)\n",
    "filtered_data2.to_csv(path2/'ir3_rna_step2.csv', index=True)\n",
    "\n",
    "\n",
    "diagnosis = pd.read_csv(path2/'patients_HC_PK_diagnosis.csv')\n",
    "patnn=diagnosis.set_index(\"PATNO\")\n",
    "# renaming the column as \"condition\" is necessary for deseq transformation.\n",
    "patnn.rename(columns={'COHORT_DEFINITION': 'condition'}, inplace=True)\n",
    "patnn.index = patnn.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is to make a dese data set:\n",
    "dds = DeseqDataSet(\n",
    "    counts=filtered_data2,\n",
    "    clinical=patnn,\n",
    "    design_factors=\"condition\"\n",
    ")\n",
    "#dds.obs # show patients diagnosis\n",
    "#dds.X # show array of counts\n",
    "# dds.var # show Geneids\n",
    "\n",
    "# Perform VST transformation\n",
    "dds.vst()\n",
    "\n",
    "# Here we get the VST data which are in the numpy form.\n",
    "vst_transformed_dds=dds.layers[\"vst_counts\"]\n",
    "\n",
    "# We convert the numpy data to pandas dataframe\n",
    "pd_vst= pd.DataFrame(vst_transformed_dds)\n",
    "\n",
    "# the above file does not have patient IDs name as well as Gene IDs so we need to take it from the other\n",
    "# file and then add it to bare dataframe file\n",
    "\n",
    "ir3_rna_step2 = pd.read_csv(path2/'ir3_rna_step2.csv')\n",
    "# patient IDs \n",
    "patient_ids = ir3_rna_step2['Unnamed: 0']\n",
    "\n",
    "# set them as the index of rows: \n",
    "pd_vst.set_index(patient_ids, inplace = True)\n",
    "\n",
    "# taking the gene IDs properly as a list format\n",
    "geneids = list(ir3_rna_step2.columns)[1:]\n",
    "\n",
    "# add the list of columns to the pandas dataframe file:\n",
    "pd_vst.columns = geneids\n",
    "# Saving the matrix with vst applied into csv file.\n",
    "pd_vst.to_csv(path2/'ir3_rna_step_vst.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d6f91",
   "metadata": {},
   "source": [
    "# RIN and SEX effects to be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4df503",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_rin_data = pd.read_csv(path2/'Patient_IDs_RNA_sample_RIN_sex_CNO_diagnosis.txt',delimiter='\\t')\n",
    "sex_rin_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = sex_rin_data['CLINICAL_EVENT'].unique()\n",
    "unique_values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf29734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to keep only base line visit data of patient.\n",
    "#baseline_df = sex_rin_data[sex_rin_data['CLINICAL_EVENT'] == 'BL']\n",
    "# Filter the DataFrame to keep rows with 'BL' and 'SC' values in the 'CLINICAL_EVENT' column\n",
    "baseline_df = sex_rin_data[sex_rin_data['CLINICAL_EVENT'].isin(['BL'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis = pd.read_csv(path2/'patients_HC_PK_diagnosis.csv')\n",
    "diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = diagnosis['PATNO']\n",
    "filtered_df_sex_rin = baseline_df[baseline_df['ALIAS_ID'].isin(patient_ids)]\n",
    "filtered_df_sex_rin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73fb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all patient IDs in diagnosis exist in filtered_df_sex_rin\n",
    "all_exist = diagnosis['PATNO'].isin(baseline_df['ALIAS_ID']).all()\n",
    "\n",
    "# Print the result\n",
    "if all_exist:\n",
    "    print(\"All patient IDs in other_df exist in big_df.\")\n",
    "else:\n",
    "    print(\"Not all patient IDs in other_df exist in big_df.\")\n",
    "# I note that all the patient IDs that I analysis are note only in BL some of them are in V01.\n",
    "# I need to find those that are in the v01 and only add them not the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679dba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = filtered_df_sex_rin['ALIAS_ID'].duplicated()\n",
    "duplicate_rows = filtered_df_sex_rin[duplicates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d01204",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicated patient IDs in the filtered DataFrame\n",
    "duplicates = filtered_df_sex_rin['ALIAS_ID'].duplicated(keep=False)\n",
    "\n",
    "# Filter the DataFrame to keep only the duplicated rows\n",
    "duplicate_rows = filtered_df_sex_rin[duplicates]\n",
    "\n",
    "# Sort the duplicate rows by the patient ID\n",
    "duplicate_rows_sorted = duplicate_rows.sort_values('ALIAS_ID')\n",
    "\n",
    "# Print the duplicate rows\n",
    "duplicate_rows_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5d06c",
   "metadata": {},
   "source": [
    "<a id=\"training\"></a>\n",
    "## Model Training \n",
    "\n",
    "Build and train machine learning models on the prepared data. Explain the choice of models, feature engineering techniques, and hyperparameter tuning. Provide code and comments to walk through the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6376df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 530 genes:\n",
    "ir3_rna_step_vst =  pd.read_csv(path2/'mydata_Log_CPM_filtered_bact_sex_effect_removed_RIN_covariate_top_530_limma.txt',delimiter='\\t' )\n",
    "\n",
    "ir3_rna_step_vst.rename(columns={\"Unnamed: 0\": \"PATNO\"}, inplace=True)\n",
    "\n",
    "ir3_rna_step_vst.set_index(\"PATNO\", inplace = True)\n",
    "\n",
    "diagnosis = pd.read_csv(path2/'patients_HC_PK_diagnosis.csv')\n",
    "\n",
    "# mapping diagnosis to zero and one.\n",
    "diagnosis['COHORT_DEFINITION'] = diagnosis['COHORT_DEFINITION'].map({'Healthy Control': 0, \"Parkinson's Disease\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adadd4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f38df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with all the filtered genes:\n",
    "ir3_rna_step_vst =  pd.read_csv(path2/'mydata_Log_CPM_filtered_bact_sex_effect_removed_RIN_covariate.txt',delimiter='\\t' )\n",
    "\n",
    "\n",
    "\n",
    "diagnosis = pd.read_csv(path2/'patients_HC_PK_diagnosis.csv')\n",
    "\n",
    "# mapping diagnosis to zero and one.\n",
    "diagnosis['COHORT_DEFINITION'] = diagnosis['COHORT_DEFINITION'].map({'Healthy Control': 0, \"Parkinson's Disease\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5975f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3000</th>\n",
       "      <th>3001</th>\n",
       "      <th>3002</th>\n",
       "      <th>3003</th>\n",
       "      <th>3004</th>\n",
       "      <th>3006</th>\n",
       "      <th>3007</th>\n",
       "      <th>3008</th>\n",
       "      <th>3009</th>\n",
       "      <th>3010</th>\n",
       "      <th>...</th>\n",
       "      <th>4121</th>\n",
       "      <th>4122</th>\n",
       "      <th>4123</th>\n",
       "      <th>4124</th>\n",
       "      <th>4125</th>\n",
       "      <th>4126</th>\n",
       "      <th>4135</th>\n",
       "      <th>4136</th>\n",
       "      <th>4139</th>\n",
       "      <th>41410</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000000419</th>\n",
       "      <td>3.845119</td>\n",
       "      <td>5.213262</td>\n",
       "      <td>4.837730</td>\n",
       "      <td>5.101270</td>\n",
       "      <td>4.974751</td>\n",
       "      <td>4.825708</td>\n",
       "      <td>5.011214</td>\n",
       "      <td>5.229956</td>\n",
       "      <td>5.174531</td>\n",
       "      <td>5.284571</td>\n",
       "      <td>...</td>\n",
       "      <td>4.451699</td>\n",
       "      <td>5.408345</td>\n",
       "      <td>5.247058</td>\n",
       "      <td>4.584573</td>\n",
       "      <td>4.398598</td>\n",
       "      <td>5.449524</td>\n",
       "      <td>5.015613</td>\n",
       "      <td>3.987076</td>\n",
       "      <td>5.496642</td>\n",
       "      <td>4.963686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000000457</th>\n",
       "      <td>5.648852</td>\n",
       "      <td>6.202708</td>\n",
       "      <td>5.624314</td>\n",
       "      <td>6.067816</td>\n",
       "      <td>6.092646</td>\n",
       "      <td>6.223112</td>\n",
       "      <td>6.003440</td>\n",
       "      <td>6.010872</td>\n",
       "      <td>6.272480</td>\n",
       "      <td>6.285266</td>\n",
       "      <td>...</td>\n",
       "      <td>5.863215</td>\n",
       "      <td>6.422896</td>\n",
       "      <td>6.368654</td>\n",
       "      <td>6.114092</td>\n",
       "      <td>5.798945</td>\n",
       "      <td>6.383039</td>\n",
       "      <td>6.290690</td>\n",
       "      <td>5.179048</td>\n",
       "      <td>6.293745</td>\n",
       "      <td>6.058590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000000460</th>\n",
       "      <td>3.817007</td>\n",
       "      <td>4.095830</td>\n",
       "      <td>4.012751</td>\n",
       "      <td>4.262379</td>\n",
       "      <td>3.969580</td>\n",
       "      <td>4.374808</td>\n",
       "      <td>4.417553</td>\n",
       "      <td>4.331596</td>\n",
       "      <td>4.675491</td>\n",
       "      <td>4.348990</td>\n",
       "      <td>...</td>\n",
       "      <td>3.957820</td>\n",
       "      <td>4.518421</td>\n",
       "      <td>4.424952</td>\n",
       "      <td>4.145217</td>\n",
       "      <td>4.194468</td>\n",
       "      <td>4.628795</td>\n",
       "      <td>4.747158</td>\n",
       "      <td>3.569694</td>\n",
       "      <td>4.025577</td>\n",
       "      <td>4.268701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000000938</th>\n",
       "      <td>8.463275</td>\n",
       "      <td>8.810743</td>\n",
       "      <td>8.368607</td>\n",
       "      <td>8.466923</td>\n",
       "      <td>8.927912</td>\n",
       "      <td>9.043402</td>\n",
       "      <td>8.681277</td>\n",
       "      <td>8.235545</td>\n",
       "      <td>8.067889</td>\n",
       "      <td>8.631065</td>\n",
       "      <td>...</td>\n",
       "      <td>8.416943</td>\n",
       "      <td>9.104202</td>\n",
       "      <td>8.838484</td>\n",
       "      <td>9.193292</td>\n",
       "      <td>8.217039</td>\n",
       "      <td>8.772340</td>\n",
       "      <td>8.497247</td>\n",
       "      <td>8.612951</td>\n",
       "      <td>8.695999</td>\n",
       "      <td>8.676493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000000971</th>\n",
       "      <td>1.944928</td>\n",
       "      <td>3.384911</td>\n",
       "      <td>2.522713</td>\n",
       "      <td>4.085667</td>\n",
       "      <td>3.693875</td>\n",
       "      <td>2.544924</td>\n",
       "      <td>3.968554</td>\n",
       "      <td>4.085922</td>\n",
       "      <td>4.811027</td>\n",
       "      <td>4.093739</td>\n",
       "      <td>...</td>\n",
       "      <td>2.209538</td>\n",
       "      <td>2.390307</td>\n",
       "      <td>3.974330</td>\n",
       "      <td>2.196031</td>\n",
       "      <td>2.196303</td>\n",
       "      <td>3.910535</td>\n",
       "      <td>4.403068</td>\n",
       "      <td>2.359305</td>\n",
       "      <td>2.445675</td>\n",
       "      <td>3.267041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000285219</th>\n",
       "      <td>2.721748</td>\n",
       "      <td>1.638440</td>\n",
       "      <td>3.771521</td>\n",
       "      <td>1.812421</td>\n",
       "      <td>2.549122</td>\n",
       "      <td>2.058582</td>\n",
       "      <td>2.125234</td>\n",
       "      <td>1.765596</td>\n",
       "      <td>2.069582</td>\n",
       "      <td>1.764143</td>\n",
       "      <td>...</td>\n",
       "      <td>2.495753</td>\n",
       "      <td>1.830501</td>\n",
       "      <td>1.218611</td>\n",
       "      <td>2.165035</td>\n",
       "      <td>3.126425</td>\n",
       "      <td>1.943526</td>\n",
       "      <td>0.648852</td>\n",
       "      <td>2.959793</td>\n",
       "      <td>2.179546</td>\n",
       "      <td>2.136740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000285230</th>\n",
       "      <td>2.322872</td>\n",
       "      <td>2.517009</td>\n",
       "      <td>2.599599</td>\n",
       "      <td>2.386956</td>\n",
       "      <td>2.851771</td>\n",
       "      <td>2.737816</td>\n",
       "      <td>2.894924</td>\n",
       "      <td>2.388526</td>\n",
       "      <td>2.458358</td>\n",
       "      <td>2.695723</td>\n",
       "      <td>...</td>\n",
       "      <td>2.169890</td>\n",
       "      <td>2.481177</td>\n",
       "      <td>2.779666</td>\n",
       "      <td>2.643108</td>\n",
       "      <td>2.415556</td>\n",
       "      <td>2.438769</td>\n",
       "      <td>2.464313</td>\n",
       "      <td>2.474065</td>\n",
       "      <td>2.487454</td>\n",
       "      <td>2.544368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000285280</th>\n",
       "      <td>4.541140</td>\n",
       "      <td>1.709681</td>\n",
       "      <td>2.196385</td>\n",
       "      <td>2.727775</td>\n",
       "      <td>2.760335</td>\n",
       "      <td>2.204436</td>\n",
       "      <td>2.655007</td>\n",
       "      <td>2.044956</td>\n",
       "      <td>1.865374</td>\n",
       "      <td>1.846651</td>\n",
       "      <td>...</td>\n",
       "      <td>2.344181</td>\n",
       "      <td>2.573584</td>\n",
       "      <td>2.253876</td>\n",
       "      <td>2.459218</td>\n",
       "      <td>3.058924</td>\n",
       "      <td>2.916741</td>\n",
       "      <td>2.501802</td>\n",
       "      <td>2.977444</td>\n",
       "      <td>2.196541</td>\n",
       "      <td>2.727432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000285399</th>\n",
       "      <td>5.099339</td>\n",
       "      <td>5.353763</td>\n",
       "      <td>4.708896</td>\n",
       "      <td>5.048748</td>\n",
       "      <td>5.349433</td>\n",
       "      <td>5.567314</td>\n",
       "      <td>5.192088</td>\n",
       "      <td>5.066864</td>\n",
       "      <td>5.211345</td>\n",
       "      <td>5.326240</td>\n",
       "      <td>...</td>\n",
       "      <td>5.208116</td>\n",
       "      <td>5.197208</td>\n",
       "      <td>5.421681</td>\n",
       "      <td>5.656614</td>\n",
       "      <td>5.370812</td>\n",
       "      <td>5.384910</td>\n",
       "      <td>5.832674</td>\n",
       "      <td>4.774830</td>\n",
       "      <td>4.800063</td>\n",
       "      <td>5.287198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000285410</th>\n",
       "      <td>5.241609</td>\n",
       "      <td>6.019862</td>\n",
       "      <td>5.414418</td>\n",
       "      <td>5.555605</td>\n",
       "      <td>5.554071</td>\n",
       "      <td>5.654699</td>\n",
       "      <td>5.836791</td>\n",
       "      <td>5.971206</td>\n",
       "      <td>6.017686</td>\n",
       "      <td>6.123887</td>\n",
       "      <td>...</td>\n",
       "      <td>5.526208</td>\n",
       "      <td>6.105318</td>\n",
       "      <td>5.998763</td>\n",
       "      <td>5.768046</td>\n",
       "      <td>5.472123</td>\n",
       "      <td>5.950408</td>\n",
       "      <td>5.878510</td>\n",
       "      <td>5.187680</td>\n",
       "      <td>5.677015</td>\n",
       "      <td>5.755279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13104 rows × 583 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     3000      3001      3002      3003      3004      3006  \\\n",
       "ENSG00000000419  3.845119  5.213262  4.837730  5.101270  4.974751  4.825708   \n",
       "ENSG00000000457  5.648852  6.202708  5.624314  6.067816  6.092646  6.223112   \n",
       "ENSG00000000460  3.817007  4.095830  4.012751  4.262379  3.969580  4.374808   \n",
       "ENSG00000000938  8.463275  8.810743  8.368607  8.466923  8.927912  9.043402   \n",
       "ENSG00000000971  1.944928  3.384911  2.522713  4.085667  3.693875  2.544924   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000285219  2.721748  1.638440  3.771521  1.812421  2.549122  2.058582   \n",
       "ENSG00000285230  2.322872  2.517009  2.599599  2.386956  2.851771  2.737816   \n",
       "ENSG00000285280  4.541140  1.709681  2.196385  2.727775  2.760335  2.204436   \n",
       "ENSG00000285399  5.099339  5.353763  4.708896  5.048748  5.349433  5.567314   \n",
       "ENSG00000285410  5.241609  6.019862  5.414418  5.555605  5.554071  5.654699   \n",
       "\n",
       "                     3007      3008      3009      3010  ...      4121  \\\n",
       "ENSG00000000419  5.011214  5.229956  5.174531  5.284571  ...  4.451699   \n",
       "ENSG00000000457  6.003440  6.010872  6.272480  6.285266  ...  5.863215   \n",
       "ENSG00000000460  4.417553  4.331596  4.675491  4.348990  ...  3.957820   \n",
       "ENSG00000000938  8.681277  8.235545  8.067889  8.631065  ...  8.416943   \n",
       "ENSG00000000971  3.968554  4.085922  4.811027  4.093739  ...  2.209538   \n",
       "...                   ...       ...       ...       ...  ...       ...   \n",
       "ENSG00000285219  2.125234  1.765596  2.069582  1.764143  ...  2.495753   \n",
       "ENSG00000285230  2.894924  2.388526  2.458358  2.695723  ...  2.169890   \n",
       "ENSG00000285280  2.655007  2.044956  1.865374  1.846651  ...  2.344181   \n",
       "ENSG00000285399  5.192088  5.066864  5.211345  5.326240  ...  5.208116   \n",
       "ENSG00000285410  5.836791  5.971206  6.017686  6.123887  ...  5.526208   \n",
       "\n",
       "                     4122      4123      4124      4125      4126      4135  \\\n",
       "ENSG00000000419  5.408345  5.247058  4.584573  4.398598  5.449524  5.015613   \n",
       "ENSG00000000457  6.422896  6.368654  6.114092  5.798945  6.383039  6.290690   \n",
       "ENSG00000000460  4.518421  4.424952  4.145217  4.194468  4.628795  4.747158   \n",
       "ENSG00000000938  9.104202  8.838484  9.193292  8.217039  8.772340  8.497247   \n",
       "ENSG00000000971  2.390307  3.974330  2.196031  2.196303  3.910535  4.403068   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000285219  1.830501  1.218611  2.165035  3.126425  1.943526  0.648852   \n",
       "ENSG00000285230  2.481177  2.779666  2.643108  2.415556  2.438769  2.464313   \n",
       "ENSG00000285280  2.573584  2.253876  2.459218  3.058924  2.916741  2.501802   \n",
       "ENSG00000285399  5.197208  5.421681  5.656614  5.370812  5.384910  5.832674   \n",
       "ENSG00000285410  6.105318  5.998763  5.768046  5.472123  5.950408  5.878510   \n",
       "\n",
       "                     4136      4139     41410  \n",
       "ENSG00000000419  3.987076  5.496642  4.963686  \n",
       "ENSG00000000457  5.179048  6.293745  6.058590  \n",
       "ENSG00000000460  3.569694  4.025577  4.268701  \n",
       "ENSG00000000938  8.612951  8.695999  8.676493  \n",
       "ENSG00000000971  2.359305  2.445675  3.267041  \n",
       "...                   ...       ...       ...  \n",
       "ENSG00000285219  2.959793  2.179546  2.136740  \n",
       "ENSG00000285230  2.474065  2.487454  2.544368  \n",
       "ENSG00000285280  2.977444  2.196541  2.727432  \n",
       "ENSG00000285399  4.774830  4.800063  5.287198  \n",
       "ENSG00000285410  5.187680  5.677015  5.755279  \n",
       "\n",
       "[13104 rows x 583 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir3_rna_step_vst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b25a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir3_rna_step_vst=ir3_rna_step_vst.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912930c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir3_rna_step_vst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see how many Parkinson and Healthy patients we have\n",
    "diagnosis['COHORT_DEFINITION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c57a378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Metrics\n",
      "Accuracy:   0.6571428571428571\n",
      "Precision:  0.7056798721497325\n",
      "Recall:     0.8613269808228114\n",
      "F1:         0.7750792544920289\n",
      "ROC-AUC:    0.5365737601655333\n",
      "\n",
      "Average Confusion Matrix\n",
      " [[ 11.5  43.3]\n",
      " [ 16.7 103.5]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluation metric lists\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Perform the iterations\n",
    "for _ in range(20):\n",
    "    # Split the data into training and test sets\n",
    "    # X_train: train data of RNA,  X_test: test data of RNA \n",
    "    # y_train: train diagnosis, y_test: test diagnosis\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ir3_rna_step_vst, diagnosis['COHORT_DEFINITION'], test_size=.3)\n",
    "    \n",
    "    # Create and fit the Random Forest model\n",
    "   # model_rf = RandomForestClassifier(n_estimators=100)\n",
    "    model_rf = XGBClassifier(n_estimators=100)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model and store the metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, model_rf.predict(X_test)))\n",
    "    precision_scores.append(precision_score(y_test, model_rf.predict(X_test)))\n",
    "    recall_scores.append(recall_score(y_test, model_rf.predict(X_test)))\n",
    "    f1_scores.append(f1_score(y_test, model_rf.predict(X_test)))\n",
    "    roc_auc_scores.append(roc_auc_score(y_test, model_rf.predict(X_test)))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, model_rf.predict(X_test)))\n",
    "\n",
    "# Calculate the average evaluation metrics\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_roc_auc = np.mean(roc_auc_scores)\n",
    "avg_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "# Print the average evaluation metrics\n",
    "print('Average Evaluation Metrics')\n",
    "print('Accuracy:  ', avg_accuracy)\n",
    "print('Precision: ', avg_precision)\n",
    "print('Recall:    ', avg_recall)\n",
    "print('F1:        ', avg_f1)\n",
    "print('ROC-AUC:   ', avg_roc_auc)\n",
    "print('\\nAverage Confusion Matrix\\n', avg_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'ir3_rna_step_vst' is my RNA-seq data and 'diagnosis' is the corresponding labels\n",
    "\n",
    "# Set the number of repetitions for Random Forest training\n",
    "num_repetitions = 1\n",
    "\n",
    "# Set the number of folds for repeated stratified 10-fold cross-validation\n",
    "num_folds = 10\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Define the function to train a Random Forest model and compute permutation feature importances\n",
    "def train_rf_with_permutation_importance(X_train, y_train, X_test, y_test, num_features):\n",
    "    rf_model = RandomForestClassifier(n_estimators=1000, random_state=random_seed)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=20, random_state=random_seed)\n",
    "    feature_importance = perm_importance.importances_mean\n",
    "\n",
    "    # Get the indices of the top features based on their importance\n",
    "    top_feature_indices = np.argsort(feature_importance)[-int(np.sqrt(num_features)):]\n",
    "    \n",
    "    return top_feature_indices\n",
    "\n",
    "# Initialize lists to store the selected top features across all splits\n",
    "selected_top_features = []\n",
    "\n",
    "# Create the repeated stratified k-fold cross-validator\n",
    "cv = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_seed)\n",
    "\n",
    "# Repeat the process for a total of 'num_repetitions' times\n",
    "for repetition in range(num_repetitions):\n",
    "    print(f\"Repetition {repetition+1}/{num_repetitions}\")\n",
    "\n",
    "    # Perfo0rm repeated stratified k-fold cross-validation\n",
    "    for train_idx, test_idx in cv.split(ir3_rna_step_vst, diagnosis['COHORT_DEFINITION']):\n",
    "        X_train, X_test = ir3_rna_step_vst.iloc[train_idx], ir3_rna_step_vst.iloc[test_idx]\n",
    "        y_train, y_test = diagnosis['COHORT_DEFINITION'].iloc[train_idx], diagnosis['COHORT_DEFINITION'].iloc[test_idx]\n",
    "\n",
    "        # Get the selected top features for this split\n",
    "        top_features_split = train_rf_with_permutation_importance(X_train, y_train, X_test, y_test, len(ir3_rna_step_vst.columns))\n",
    "\n",
    "        # Add the selected top features to the list\n",
    "        selected_top_features.extend(top_features_split)\n",
    "\n",
    "# Count the occurrences of each feature in the selected_top_features list\n",
    "feature_counts = pd.Series(selected_top_features).value_counts()\n",
    "\n",
    "# Get the indices of the top features based on their overall occurrence\n",
    "final_top_feature_indices = feature_counts.index[:2*int(np.sqrt(len(ir3_rna_step_vst.columns)))]\n",
    "\n",
    "# Get the names of the final selected top features\n",
    "final_top_features = ir3_rna_step_vst.columns[final_top_feature_indices]\n",
    "final_top_features.to_csv(path2/\"important_features.csv\", index = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a54690",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([df, time_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "data = {\n",
    "    'Column1': [7, 2, 3],\n",
    "    'Column2':  [87, 2, 3],\n",
    "    'Column3':  [1, 23, 31]\n",
    "}\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "bb = 100*df\n",
    "end_time = time.time()\n",
    "computation_time = end_time - start_time\n",
    "time_df = pd.DataFrame({'Computation Time (seconds)': [computation_time]})\n",
    "result_df = pd.concat([bb, time_df], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have X_train and y_train prepared\n",
    "X_train, X_test, y_train, y_test = train_test_split(ir3_rna_step_vst, diagnosis['COHORT_DEFINITION'], test_size=.3)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Sort the importances in descending order\n",
    "sorted_indices = importances.argsort()[::-1]\n",
    "sorted_importances = importances[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Select only the top 50 features\n",
    "top_50_indices = sorted_indices[:50]\n",
    "top_50_importances = sorted_importances[:50]\n",
    "top_50_feature_names = sorted_feature_names[:50]\n",
    "\n",
    "# Visualize the top 50 feature importances\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(range(len(top_50_importances)), top_50_importances, tick_label=top_50_feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 50 Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Select the top 50 features from the original dataset\n",
    "X_train_top_50 = X_train[top_50_feature_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac736d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27602ca3",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## Results and Evaluation \n",
    "\n",
    "Present the results of the trained models, including performance metrics, accuracy, or any relevant evaluation measures. Interpret the findings and discuss the implications. Include visualizations or tables to support the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "# Initialize lists to store the ROC and precision-recall data\n",
    "roc_curves = []\n",
    "precision_recall_curves = []\n",
    "\n",
    "# Perform the iterations\n",
    "for _ in range(20):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(rna_ir3, diagnosis['COHORT_DEFINITION'], test_size=.3)\n",
    "    \n",
    "    # Create and fit the Random Forest model\n",
    "    model_rf = RandomForestClassifier(n_estimators=100)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
    "    roc_curves.append((fpr, tpr))\n",
    "    \n",
    "    # Calculate the precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, model_rf.predict_proba(X_test)[:, 1])\n",
    "    precision_recall_curves.append((precision, recall))\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for fpr, tpr in roc_curves:\n",
    "    plt.plot(fpr, tpr, lw=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for precision, recall in precision_recall_curves:\n",
    "    plt.plot(recall, precision, lw=1)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60138fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting roc curve and precision recall curve\n",
    "roc = roc_curve(y_test,model_rf.predict_proba(X_test)[:,1])\n",
    "pr  = precision_recall_curve(y_test,model_rf.predict_proba(X_test)[:,1])\n",
    "\n",
    "f = plt.figure(figsize=(20,7))\n",
    "ax = f.add_subplot(121)\n",
    "ax.plot(roc[0],roc[1])\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.grid(which='both')\n",
    "ax = f.add_subplot(122)\n",
    "ax.plot(pr[1],pr[0])\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_title('PR-curve')\n",
    "ax.grid(which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f773451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
