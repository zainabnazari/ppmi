{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53b3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you do not have following packages installed, uncomment instalisation.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#!pip install dask[complete];\n",
    "# you need to run these in case dask gives you error, it might need update.\n",
    "#!pip install --upgrade pandas \"dask[complete]\"\n",
    "#python -m pip install \"dask[dataframe]\" --upgrade\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance       \n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#!pip3 install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#!pip install conorm\n",
    "#import conorm # for tmm normalisation\n",
    "\n",
    "#!pip3 install pydeseq2 or pip install pydeseq2\n",
    "#from pydeseq2.dds import DeseqDataSet\n",
    "#from pydeseq2.ds import DeseqStats\n",
    "#from pydeseq2.utils import load_example_data\n",
    "\n",
    "\n",
    "\n",
    "#to install R :\n",
    "#conda install -c r r-irkernel\n",
    "\n",
    "#to install a library from R\n",
    "#!pip install library edgeR\n",
    "# pip install rpy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6436b65",
   "metadata": {},
   "source": [
    "### Data for Imaging of Parkinson ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c33c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path2 = Path(\"/Users/zainabnazari/data\") # in your laptop, where the output data will be saved at the end. TAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a33e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = Path(\"/home/znazari/data\") # in your laptop, where the output data will be saved at the end. TAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a5670",
   "metadata": {},
   "source": [
    "DATSCAN_LIGAND = Signal CAUDATE_R\tand else different part of the brain  6 different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f295bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging = pd.read_csv(path2/\"DaTScan_Analysis_only_SC.txt\",delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870ab7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>PATNO</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>DATSCAN_LIGAND</th>\n",
       "      <th>DATSCAN_DATE</th>\n",
       "      <th>DATSCAN_CAUDATE_R</th>\n",
       "      <th>DATSCAN_CAUDATE_L</th>\n",
       "      <th>DATSCAN_PUTAMEN_R</th>\n",
       "      <th>DATSCAN_PUTAMEN_L</th>\n",
       "      <th>DATSCAN_PUTAMEN_R_ANT</th>\n",
       "      <th>DATSCAN_PUTAMEN_L_ANT</th>\n",
       "      <th>DATSCAN_ANALYZED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Healthy Control</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apr-13</td>\n",
       "      <td>3.28</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.93</td>\n",
       "      <td>3.33</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Healthy Control</td>\n",
       "      <td>1</td>\n",
       "      <td>3008</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>set-10</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.16</td>\n",
       "      <td>1.42</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Healthy Control</td>\n",
       "      <td>1</td>\n",
       "      <td>3009</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ott-10</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.45</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Healthy Control</td>\n",
       "      <td>1</td>\n",
       "      <td>3011</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nov-10</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.67</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healthy Control</td>\n",
       "      <td>1</td>\n",
       "      <td>3013</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nov-10</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.25</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>162929</td>\n",
       "      <td>SC</td>\n",
       "      <td>123I-DaTscan</td>\n",
       "      <td>mar-22</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>162994</td>\n",
       "      <td>SC</td>\n",
       "      <td>123I-DaTscan</td>\n",
       "      <td>apr-22</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.03</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>163265</td>\n",
       "      <td>SC</td>\n",
       "      <td>123I-DaTscan</td>\n",
       "      <td>mar-22</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.75</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>164900</td>\n",
       "      <td>SC</td>\n",
       "      <td>123I-DaTscan</td>\n",
       "      <td>dic-21</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.36</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>Parkinson's Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>167222</td>\n",
       "      <td>SC</td>\n",
       "      <td>123I-DaTscan</td>\n",
       "      <td>mar-22</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.04</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1243 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Diagnosis  PROTOCOL   PATNO EVENT_ID DATSCAN_LIGAND  \\\n",
       "0         Healthy Control         1    3000       SC            NaN   \n",
       "1         Healthy Control         1    3008       SC            NaN   \n",
       "2         Healthy Control         1    3009       SC            NaN   \n",
       "3         Healthy Control         1    3011       SC            NaN   \n",
       "4         Healthy Control         1    3013       SC            NaN   \n",
       "...                   ...       ...     ...      ...            ...   \n",
       "1238  Parkinson's Disease         2  162929       SC   123I-DaTscan   \n",
       "1239  Parkinson's Disease         2  162994       SC   123I-DaTscan   \n",
       "1240  Parkinson's Disease         2  163265       SC   123I-DaTscan   \n",
       "1241  Parkinson's Disease         2  164900       SC   123I-DaTscan   \n",
       "1242  Parkinson's Disease         2  167222       SC   123I-DaTscan   \n",
       "\n",
       "     DATSCAN_DATE  DATSCAN_CAUDATE_R  DATSCAN_CAUDATE_L  DATSCAN_PUTAMEN_R  \\\n",
       "0          apr-13               3.28               3.20               2.53   \n",
       "1          set-10               2.13               1.82               1.75   \n",
       "2          ott-10               1.73               1.93               0.95   \n",
       "3          nov-10               1.91               1.04               1.12   \n",
       "4          nov-10               2.11               2.26               1.15   \n",
       "...           ...                ...                ...                ...   \n",
       "1238       mar-22               1.67               2.07               0.76   \n",
       "1239       apr-22               1.22               1.39               0.38   \n",
       "1240       mar-22               1.60               2.32               0.58   \n",
       "1241       dic-21               2.88               2.75               1.89   \n",
       "1242       mar-22               1.18               1.47               0.53   \n",
       "\n",
       "      DATSCAN_PUTAMEN_L  DATSCAN_PUTAMEN_R_ANT  DATSCAN_PUTAMEN_L_ANT  \\\n",
       "0                  2.71                   2.93                   3.33   \n",
       "1                  1.04                   2.16                   1.42   \n",
       "2                  0.74                   1.30                   1.45   \n",
       "3                  0.43                   1.44                   0.67   \n",
       "4                  1.58                   1.56                   2.25   \n",
       "...                 ...                    ...                    ...   \n",
       "1238               0.47                   1.04                   1.00   \n",
       "1239               0.73                   0.66                   1.03   \n",
       "1240               1.35                   1.05                   1.75   \n",
       "1241               1.89                   2.39                   2.36   \n",
       "1242               0.49                   0.71                   1.04   \n",
       "\n",
       "     DATSCAN_ANALYZED  \n",
       "0                 Yes  \n",
       "1                 Yes  \n",
       "2                 Yes  \n",
       "3                 Yes  \n",
       "4                 Yes  \n",
       "...               ...  \n",
       "1238              Yes  \n",
       "1239              Yes  \n",
       "1240              Yes  \n",
       "1241              Yes  \n",
       "1242              Yes  \n",
       "\n",
       "[1243 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f64b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove=['PROTOCOL', 'EVENT_ID','DATSCAN_LIGAND','DATSCAN_DATE','DATSCAN_ANALYZED']\n",
    "imaging_=imaging.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d915d98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SC'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaging['EVENT_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d5061b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           Healthy Control\n",
       "1           Healthy Control\n",
       "2           Healthy Control\n",
       "3           Healthy Control\n",
       "4           Healthy Control\n",
       "               ...         \n",
       "1238    Parkinson's Disease\n",
       "1239    Parkinson's Disease\n",
       "1240    Parkinson's Disease\n",
       "1241    Parkinson's Disease\n",
       "1242    Parkinson's Disease\n",
       "Name: Diagnosis, Length: 1243, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaging['Diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cecddb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Healthy Control', \"Parkinson's Disease\"], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imaging_['Diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d2ed4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numeric values using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(imaging['Diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8d103ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d8b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imaging_.drop(['PATNO', 'Diagnosis'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "312510a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd9e0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62397f90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5346/2508776691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create and train the XGBoost model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model = XGBClassifier()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1197\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Create and train the XGBoost model\n",
    "#model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfa4deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities on the test set\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a64db9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under the ROC Curve (AU-ROC) score: 0.5316127694859039\n"
     ]
    }
   ],
   "source": [
    "# Calculate AU-ROC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"Area Under the ROC Curve (AU-ROC) score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b75101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each feature and perform the Mann-Whitney U test\n",
    "for feature in imaging_.columns[:-1]:  # Exclude the 'group' column\n",
    "    group_a_values = imaging_[imaging_['DATSCAN_CAUDATE_R'] == 'Group_A'][feature]\n",
    "    group_b_values = imaging_[imaging_['group'] == 'Group_B'][feature]\n",
    "\n",
    "    # Perform Mann-Whitney U test\n",
    "    stat, p_value = mannwhitneyu(group_a_values, group_b_values, alternative='two-sided')\n",
    "\n",
    "    # Store the results\n",
    "    results.append({'Feature': feature, 'Stat': stat, 'P-value': p_value})\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ecf172f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#command+/v to uncomment \n",
    "# # I tried to find the best hyperparameter for this model but it didn't work well as the best score was not better than the default hyperparameter that was selested by the XGboost itself.\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# # Assuming df is your DataFrame with features and labels\n",
    "\n",
    "# # Feature engineering, preprocessing, and XGBoost in a pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', ColumnTransformer(transformers=[\n",
    "#         ('num', StandardScaler(), ['DATSCAN_CAUDATE_R', 'DATSCAN_CAUDATE_L', 'DATSCAN_PUTAMEN_R', 'DATSCAN_PUTAMEN_L', 'DATSCAN_PUTAMEN_R_ANT', 'DATSCAN_PUTAMEN_L_ANT']),   \n",
    "#     ])),\n",
    "#     ('classifier', XGBClassifier())\n",
    "# ])\n",
    "\n",
    "# # Define hyperparameters for grid search\n",
    "# param_grid = {\n",
    "#     'classifier__n_estimators': [400,500,600]\n",
    "# }\n",
    "\n",
    "# # Use grid search to find the best hyperparameters\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc')\n",
    "# grid_search.fit(imaging_.drop(['PATNO', 'Diagnosis'], axis=1), y)\n",
    "\n",
    "# # Display the best parameters and AU-ROC score\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best AU-ROC Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have tried to standarized the data with features using a well-known module in pyhthon \n",
    "# I noticed the performance dropped from 53 to 49 Au-Roc score !\n",
    "\n",
    "#removing noise and standardizing the features, it \n",
    "#ensures that each feature has a mean of 0 and a standard deviation of 1.\n",
    "#scaler=StandardScaler()\n",
    "#X_standardized = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
